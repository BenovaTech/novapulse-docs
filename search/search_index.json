{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NovaPulse Documentation","text":"<p>NovaPulse is an AI-powered Voice Analytics Platform that transforms raw audio conversations into structured, actionable intelligence. Built for enterprise teams in customer support, sales, HR, and healthcare, NovaPulse delivers transcription, sentiment analysis, speaker diarization, and autonomous voice agent capabilities through a single REST API.</p>"},{"location":"#quick-navigation","title":"Quick navigation","text":"New to NovaPulseBuilding with the APIUnderstanding the platform <p>Start here if you're integrating NovaPulse for the first time.</p> <ol> <li>Read the Platform Overview    to understand what NovaPulse does and how it's structured.</li> <li>Follow the Getting Started tutorial    to make your first API call in under 10 minutes.</li> <li>Explore the API Reference for complete    endpoint documentation.</li> </ol> <p>For developers actively integrating NovaPulse.</p> <ul> <li>Authentication \u2014 API keys and headers</li> <li>Audio Ingestion API \u2014 Upload audio files</li> <li>Transcription API \u2014 Retrieve transcripts</li> <li>Sentiment Analysis API \u2014 Query insights</li> </ul> <p>Conceptual guides for deeper understanding.</p> <ul> <li>Vector Search in NovaPulse</li> <li>RAG Pipeline architecture</li> <li>NLP capabilities</li> <li>System architecture</li> </ul>"},{"location":"#platform-capabilities","title":"Platform capabilities","text":"Capability Description Phase Transcription Speaker-attributed text from audio with \u226590% accuracy GA Sentiment analysis Per-segment sentiment scoring and trend detection GA Speaker diarization Automatic speaker identification and labeling GA PII redaction Automated detection and redaction of personal data GA Real-time analytics Live audio stream processing with \u22642s latency Beta Autonomous agents Context-aware AI voice agents for full conversations Planned"},{"location":"#documentation-status","title":"Documentation status","text":"Section Status Last updated API Reference  Complete 2025-05-01 Tutorials  Complete 2025-05-01 Concepts  Complete 2025-05-01 Architecture  Complete 2025-05-01 Reference  Complete 2025-05-01 Release Notes  Complete 2025-05-01 <p>About this documentation</p> <p>This documentation site is a portfolio project demonstrating technical writing methodology for an AI SaaS platform. Built with MkDocs Material, deployed via GitHub Actions, and maintained using a docs-as-code workflow. \u2014 Houda Benlemmouden</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>The NovaPulse API is a REST API that accepts JSON request bodies and returns JSON responses. All requests must be authenticated using an API key.</p> <p>Base URL: <code>https://api.novapulse.io/v1</code></p>"},{"location":"api-reference/#authentication","title":"Authentication","text":"<p>All endpoints require a Bearer token in the <code>Authorization</code> header:</p> <pre><code>Authorization: Bearer nvp_live_YOUR_API_KEY\n</code></pre> <p>See Authentication for full details.</p>"},{"location":"api-reference/#endpoints","title":"Endpoints","text":"Endpoint Method Description <code>/v1/audio/ingest</code> POST Upload an audio file for processing <code>/v1/jobs/{job_id}</code> GET Check processing job status <code>/v1/transcriptions/{job_id}</code> GET Retrieve transcript and diarization results <code>/v1/insights/{job_id}</code> GET Retrieve sentiment analysis and behavioral metrics"},{"location":"api-reference/#request-and-response-format","title":"Request and response format","text":"<p>All request bodies must be sent as <code>multipart/form-data</code> (for file uploads) or <code>application/json</code>. All responses are returned as <code>application/json</code> unless an alternative format is requested via query parameter.</p>"},{"location":"api-reference/#error-format","title":"Error format","text":"<p>All error responses follow a consistent structure:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"invalid_api_key\",\n    \"message\": \"The API key provided is invalid or has been revoked.\",\n    \"status\": 401\n  }\n}\n</code></pre> <p>See Error codes for the full list.</p>"},{"location":"api-reference/#rate-limits","title":"Rate limits","text":"<p>All endpoints are rate-limited per API key. See Rate limits for thresholds and retry guidance.</p>"},{"location":"api-reference/#api-versioning","title":"API versioning","text":"<p>The current stable version is <code>v1</code>. Breaking changes will not be introduced without a version increment and a minimum 90-day deprecation notice.</p>"},{"location":"api-reference/audio-ingestion/","title":"Audio Ingestion API","text":"<p>Upload an audio file to NovaPulse to begin the processing pipeline. The API accepts audio files in multiple formats, validates the upload, stores the file securely, and returns a job ID for tracking.</p>"},{"location":"api-reference/audio-ingestion/#endpoint","title":"Endpoint","text":"<p>POST <code>/v1/audio/ingest</code></p>"},{"location":"api-reference/audio-ingestion/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid API key with <code>audio:write</code> permission</li> <li>Audio file in a supported format (see Supported formats)</li> <li>File size must not exceed 500 MB</li> </ul>"},{"location":"api-reference/audio-ingestion/#request","title":"Request","text":"<p>Send the request as <code>multipart/form-data</code>.</p>"},{"location":"api-reference/audio-ingestion/#form-fields","title":"Form fields","text":"Field Type Required Description <code>file</code> binary Yes The audio file to upload. <code>language</code> string No BCP-47 language tag. Default: <code>en-US</code>. <code>pipeline</code> string No Processing pipeline. One of: <code>transcription</code>, <code>full-analysis</code>. Default: <code>full-analysis</code>. <code>metadata</code> JSON string No Optional key-value pairs attached to the job (e.g. <code>{\"agent_id\": \"AGT-42\"}</code>). <code>webhook_url</code> string No HTTPS URL to notify when processing completes."},{"location":"api-reference/audio-ingestion/#pipeline-values","title":"Pipeline values","text":"Value What runs <code>transcription</code> Transcription and speaker diarization only. Faster and lower cost. <code>full-analysis</code> Transcription + diarization + sentiment analysis + summarization + PII detection."},{"location":"api-reference/audio-ingestion/#supported-formats","title":"Supported formats","text":"Format Extension Max sample rate MP3 <code>.mp3</code> 48 kHz WAV <code>.wav</code> 96 kHz M4A <code>.m4a</code> 48 kHz FLAC <code>.flac</code> 96 kHz OGG <code>.ogg</code> 48 kHz"},{"location":"api-reference/audio-ingestion/#request-example","title":"Request example","text":"cURLPython <pre><code>curl -X POST https://api.novapulse.io/v1/audio/ingest \\\n  -H 'Authorization: Bearer nvp_live_abc123...' \\\n  -F 'file=@/path/to/call-recording.mp3' \\\n  -F 'pipeline=full-analysis' \\\n  -F 'language=en-US' \\\n  -F 'metadata={\"agent_id\": \"AGT-42\", \"call_type\": \"support\"}'\n</code></pre> <pre><code>import requests\nimport json\n\nurl = 'https://api.novapulse.io/v1/audio/ingest'\nheaders = {'Authorization': 'Bearer nvp_live_abc123...'}\n\nwith open('/path/to/call-recording.mp3', 'rb') as audio_file:\n    files = {'file': ('call-recording.mp3', audio_file, 'audio/mpeg')}\n    data = {\n        'pipeline': 'full-analysis',\n        'language': 'en-US',\n        'metadata': json.dumps({'agent_id': 'AGT-42'})\n    }\n    response = requests.post(url, headers=headers, files=files, data=data)\n\nprint(response.json())\n</code></pre>"},{"location":"api-reference/audio-ingestion/#response","title":"Response","text":"<p>Returns <code>202 Accepted</code> immediately. Processing happens asynchronously.</p> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"status\": \"queued\",\n  \"pipeline\": \"full-analysis\",\n  \"created_at\": \"2025-04-15T14:32:00Z\",\n  \"estimated_completion\": \"2025-04-15T14:34:00Z\",\n  \"file\": {\n    \"name\": \"call-recording.mp3\",\n    \"size_bytes\": 4823041,\n    \"duration_seconds\": 312\n  }\n}\n</code></pre>"},{"location":"api-reference/audio-ingestion/#response-fields","title":"Response fields","text":"Field Type Description <code>job_id</code> string Unique identifier for this processing job. Use this to poll status. <code>status</code> string Initial status. Always <code>queued</code> on creation. <code>pipeline</code> string The pipeline requested. <code>created_at</code> string (ISO 8601) Timestamp when the job was created. <code>estimated_completion</code> string (ISO 8601) Estimated time when results will be available. <code>file.name</code> string Original filename as uploaded. <code>file.size_bytes</code> integer File size in bytes. <code>file.duration_seconds</code> integer Audio duration detected at upload time."},{"location":"api-reference/audio-ingestion/#response-codes","title":"Response codes","text":"Code Meaning <code>202 Accepted</code> Upload successful. Job has been queued. <code>400 Bad Request</code> Missing required field or invalid parameter. <code>401 Unauthorized</code> Invalid or missing API key. <code>413 Payload Too Large</code> File exceeds 500 MB limit. <code>415 Unsupported Media Type</code> Audio format not supported. <code>429 Too Many Requests</code> Rate limit exceeded. See Rate limits."},{"location":"api-reference/audio-ingestion/#polling-for-job-status","title":"Polling for job status","text":"<p>After upload, poll <code>GET /v1/jobs/{job_id}</code> to check processing status:</p> <pre><code>curl https://api.novapulse.io/v1/jobs/job_9f3c2a1b4e7d \\\n  -H 'Authorization: Bearer nvp_live_abc123...'\n</code></pre> <p>Job status transitions: <code>queued</code> \u2192 <code>processing</code> \u2192 <code>completed</code> or <code>failed</code></p>"},{"location":"api-reference/audio-ingestion/#related","title":"Related","text":"<ul> <li>Transcription API \u2014 Retrieve transcript results</li> <li>Sentiment Analysis API \u2014 Retrieve sentiment data</li> <li>Getting Started tutorial</li> </ul>"},{"location":"api-reference/authentication/","title":"Authentication","text":"<p>The NovaPulse API uses API key authentication. Include your API key as a Bearer token in the <code>Authorization</code> header of every request.</p>"},{"location":"api-reference/authentication/#obtain-an-api-key","title":"Obtain an API key","text":"<ol> <li>Log in to the NovaPulse dashboard.</li> <li>Navigate to Settings \u2192 API Keys.</li> <li>Click Generate new key.</li> <li>Copy the key immediately, it is shown only once.</li> </ol> <p>Keep your API key secure</p> <p>Never commit API keys to source control or expose them in client-side code. Use environment variables or a secrets manager.</p>"},{"location":"api-reference/authentication/#authentication-header","title":"Authentication header","text":"<p>Include your key in every API request:</p> <pre><code>Authorization: Bearer nvp_live_YOUR_API_KEY_HERE\n</code></pre>"},{"location":"api-reference/authentication/#request-examples","title":"Request examples","text":"cURLPythonJavaScript <pre><code>curl -X GET https://api.novapulse.io/v1/jobs \\\n  -H 'Authorization: Bearer nvp_live_abc123...' \\\n  -H 'Content-Type: application/json'\n</code></pre> <pre><code>import requests\n\nheaders = {\n    'Authorization': 'Bearer nvp_live_abc123...',\n    'Content-Type': 'application/json'\n}\n\nresponse = requests.get('https://api.novapulse.io/v1/jobs', headers=headers)\n</code></pre> <pre><code>const response = await fetch('https://api.novapulse.io/v1/jobs', {\n  headers: {\n    'Authorization': 'Bearer nvp_live_abc123...',\n    'Content-Type': 'application/json'\n  }\n});\n</code></pre>"},{"location":"api-reference/authentication/#authentication-errors","title":"Authentication errors","text":"HTTP status Error code Description <code>401 Unauthorized</code> <code>invalid_api_key</code> The key is missing or malformed. <code>401 Unauthorized</code> <code>expired_api_key</code> The key has been revoked or expired. <code>403 Forbidden</code> <code>insufficient_permissions</code> The key doesn't have access to this resource."},{"location":"api-reference/authentication/#api-key-prefixes","title":"API key prefixes","text":"<p>NovaPulse API keys use a prefix to distinguish environment:</p> Prefix Environment Use for <code>nvp_live_</code> Production Live data and real integrations <code>nvp_test_</code> Sandbox Development and testing \u2014 no charges"},{"location":"api-reference/authentication/#related","title":"Related","text":"<ul> <li>Rate limits</li> <li>Error codes reference</li> <li>Getting started tutorial</li> </ul>"},{"location":"api-reference/sentiment-analysis/","title":"Sentiment Analysis API","text":"<p>Retrieve sentiment analysis results for a completed audio job. NovaPulse's sentiment engine scores each transcript segment on a continuous scale and produces an overall call sentiment classification, trend data, and behavioral signals such as silence ratio and interruption count.</p> <p>Sentiment analysis is available when the job was processed with <code>pipeline=full-analysis</code>.</p>"},{"location":"api-reference/sentiment-analysis/#endpoint","title":"Endpoint","text":"<p>GET <code>/v1/insights/{job_id}</code></p>"},{"location":"api-reference/sentiment-analysis/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid API key with <code>insights:read</code> permission</li> <li>A completed job processed with <code>pipeline=full-analysis</code></li> </ul>"},{"location":"api-reference/sentiment-analysis/#path-parameters","title":"Path parameters","text":"Parameter Type Required Description <code>job_id</code> string Yes The job ID returned by <code>POST /v1/audio/ingest</code>."},{"location":"api-reference/sentiment-analysis/#query-parameters","title":"Query parameters","text":"Parameter Type Default Description <code>type</code> string <code>all</code> Insight type to return. One of: <code>sentiment</code>, <code>behavioral</code>, <code>summary</code>, <code>all</code>. <code>speaker</code> string \u2014 Filter results to a specific speaker. Values: <code>Agent</code>, <code>Customer</code>. <code>granularity</code> string <code>segment</code> Sentiment resolution. One of: <code>segment</code>, <code>minute</code>, <code>call</code>."},{"location":"api-reference/sentiment-analysis/#request-example","title":"Request example","text":"cURL sentiment onlycURL customer sentiment onlyPython <pre><code>curl \"https://api.novapulse.io/v1/insights/job_9f3c2a1b4e7d?type=sentiment\" \\\n  -H \"Authorization: Bearer nvp_live_abc123...\"\n</code></pre> <pre><code>curl \"https://api.novapulse.io/v1/insights/job_9f3c2a1b4e7d?type=sentiment&amp;speaker=Customer\" \\\n  -H \"Authorization: Bearer nvp_live_abc123...\"\n</code></pre> <pre><code>import requests\n\njob_id = \"job_9f3c2a1b4e7d\"\nurl = f\"https://api.novapulse.io/v1/insights/{job_id}\"\nheaders = {\"Authorization\": \"Bearer nvp_live_abc123...\"}\nparams = {\"type\": \"all\", \"speaker\": \"Customer\"}\n\nresponse = requests.get(url, headers=headers, params=params)\ninsights = response.json()\n\nprint(f\"Overall sentiment: {insights['sentiment']['overall_label']}\")\nprint(f\"Score: {insights['sentiment']['overall_score']}\")\n</code></pre>"},{"location":"api-reference/sentiment-analysis/#response","title":"Response","text":"<p>Returns <code>200 OK</code> with the full insights object.</p> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"pipeline\": \"full-analysis\",\n  \"sentiment\": {\n    \"overall_label\": \"negative\",\n    \"overall_score\": -0.68,\n    \"trend\": \"declining\",\n    \"segments\": [\n      {\n        \"segment_id\": 1,\n        \"speaker\": \"Agent\",\n        \"start\": 0.0,\n        \"end\": 4.2,\n        \"label\": \"neutral\",\n        \"score\": 0.05\n      },\n      {\n        \"segment_id\": 2,\n        \"speaker\": \"Customer\",\n        \"start\": 4.5,\n        \"end\": 11.8,\n        \"label\": \"negative\",\n        \"score\": -0.82\n      },\n      {\n        \"segment_id\": 3,\n        \"speaker\": \"Agent\",\n        \"start\": 12.1,\n        \"end\": 18.4,\n        \"label\": \"neutral\",\n        \"score\": 0.12\n      }\n    ]\n  },\n  \"behavioral\": {\n    \"silence_ratio\": 0.08,\n    \"interruption_count\": 3,\n    \"agent_talk_ratio\": 0.52,\n    \"customer_talk_ratio\": 0.40,\n    \"average_response_time_seconds\": 1.4\n  },\n  \"summary\": \"Customer called to cancel subscription due to pricing concerns. Agent attempted retention. No resolution reached \u2014 escalation recommended.\",\n  \"recommendations\": [\n    \"Escalate to retention specialist \u2014 high churn risk detected.\",\n    \"Review pricing communication \u2014 customer unaware of discount options.\",\n    \"Agent response time within acceptable range.\"\n  ]\n}\n</code></pre>"},{"location":"api-reference/sentiment-analysis/#response-fields","title":"Response fields","text":""},{"location":"api-reference/sentiment-analysis/#top-level","title":"Top-level","text":"Field Type Description <code>job_id</code> string The job identifier. <code>pipeline</code> string The pipeline used. Sentiment data requires <code>full-analysis</code>. <code>sentiment</code> object Sentiment analysis results. See Sentiment object. <code>behavioral</code> object Behavioral metrics. See Behavioral object. <code>summary</code> string AI-generated call summary. <code>recommendations</code> array Actionable recommendations generated from the call analysis."},{"location":"api-reference/sentiment-analysis/#sentiment-object","title":"Sentiment object","text":"Field Type Description <code>overall_label</code> string Overall call sentiment. One of: <code>positive</code>, <code>neutral</code>, <code>negative</code>. <code>overall_score</code> float Aggregate sentiment score from -1.0 (most negative) to +1.0 (most positive). <code>trend</code> string Sentiment direction across the call. One of: <code>improving</code>, <code>stable</code>, <code>declining</code>. <code>segments</code> array Per-segment sentiment. See Sentiment segment."},{"location":"api-reference/sentiment-analysis/#sentiment-segment","title":"Sentiment segment","text":"Field Type Description <code>segment_id</code> integer References the <code>id</code> field in the transcript segment. <code>speaker</code> string Speaker label for this segment. <code>start</code> float Segment start time in seconds. <code>end</code> float Segment end time in seconds. <code>label</code> string Sentiment classification for this segment. <code>score</code> float Sentiment score for this segment (-1.0 to +1.0)."},{"location":"api-reference/sentiment-analysis/#behavioral-object","title":"Behavioral object","text":"Field Type Description <code>silence_ratio</code> float Proportion of the call spent in silence (0.0\u20131.0). High values may indicate confusion or hold time. <code>interruption_count</code> integer Number of times either speaker interrupted the other. <code>agent_talk_ratio</code> float Proportion of speaking time attributed to the agent. <code>customer_talk_ratio</code> float Proportion of speaking time attributed to the customer. <code>average_response_time_seconds</code> float Mean time between one speaker finishing and the other beginning."},{"location":"api-reference/sentiment-analysis/#sentiment-score-reference","title":"Sentiment score reference","text":"Score range Label Interpretation +0.5 to +1.0 <code>positive</code> Customer is satisfied, engaged, or pleased -0.4 to +0.5 <code>neutral</code> No strong emotional signal detected -1.0 to -0.4 <code>negative</code> Customer is frustrated, dissatisfied, or at churn risk"},{"location":"api-reference/sentiment-analysis/#response-codes","title":"Response codes","text":"Code Meaning <code>200 OK</code> Insights retrieved successfully. <code>400 Bad Request</code> Invalid <code>type</code> or <code>granularity</code> parameter. <code>401 Unauthorized</code> Invalid or missing API key. <code>403 Forbidden</code> Insufficient permissions or job processed without <code>full-analysis</code>. <code>404 Not Found</code> Job ID not found or still processing. <code>429 Too Many Requests</code> Rate limit exceeded. <p>Using sentiment for real-time alerting</p> <p>In Phase 2, NovaPulse will expose sentiment scores via a streaming endpoint for live calls. Supervisors will be able to trigger interventions when a customer's sentiment score drops below a configurable threshold during an active conversation.</p>"},{"location":"api-reference/sentiment-analysis/#related","title":"Related","text":"<ul> <li>Transcription API \u2014 Retrieve the full transcript</li> <li>Audio Ingestion API \u2014 Upload audio files</li> <li>Getting Started tutorial</li> <li>Glossary \u2014 Sentiment analysis</li> </ul>"},{"location":"api-reference/transcription/","title":"Transcription API","text":"<p>Retrieve the transcript generated from a processed audio file. Transcripts include speaker-attributed segments with timestamps, confidence scores, and word-level detail. Transcription is available after the job status is <code>completed</code>.</p>"},{"location":"api-reference/transcription/#endpoint","title":"Endpoint","text":"<p>GET <code>/v1/transcriptions/{job_id}</code></p>"},{"location":"api-reference/transcription/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid API key with <code>transcription:read</code> permission</li> <li>A completed job ID from the Audio Ingestion API</li> <li>Job status must be <code>completed</code> . Polling a job still in <code>processing</code> returns <code>404</code></li> </ul>"},{"location":"api-reference/transcription/#path-parameters","title":"Path parameters","text":"Parameter Type Required Description <code>job_id</code> string Yes The job ID returned by <code>POST /v1/audio/ingest</code>."},{"location":"api-reference/transcription/#query-parameters","title":"Query parameters","text":"Parameter Type Default Description <code>format</code> string <code>json</code> Response format. One of: <code>json</code>, <code>srt</code>, <code>vtt</code>, <code>txt</code>. <code>speaker_labels</code> boolean <code>true</code> Include speaker labels in the response. Set to <code>false</code> for raw transcript only. <code>word_timestamps</code> boolean <code>false</code> Include per-word start and end timestamps. Increases response size significantly. <code>redact_pii</code> boolean <code>true</code> Return the redacted transcript. Set to <code>false</code> to retrieve the unredacted version (requires <code>pii:read</code> permission)."},{"location":"api-reference/transcription/#request-example","title":"Request example","text":"cURLcURL SRT formatPython <pre><code>curl https://api.novapulse.io/v1/transcriptions/job_9f3c2a1b4e7d \\\n  -H \"Authorization: Bearer nvp_live_abc123...\"\n</code></pre> <pre><code>curl \"https://api.novapulse.io/v1/transcriptions/job_9f3c2a1b4e7d?format=srt\" \\\n  -H \"Authorization: Bearer nvp_live_abc123...\"\n</code></pre> <pre><code>import requests\n\njob_id = \"job_9f3c2a1b4e7d\"\nurl = f\"https://api.novapulse.io/v1/transcriptions/{job_id}\"\nheaders = {\"Authorization\": \"Bearer nvp_live_abc123...\"}\nparams = {\"word_timestamps\": True}\n\nresponse = requests.get(url, headers=headers, params=params)\ntranscript = response.json()\n</code></pre>"},{"location":"api-reference/transcription/#response","title":"Response","text":"<p>Returns <code>200 OK</code> with the transcript object.</p> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"status\": \"completed\",\n  \"language\": \"en-US\",\n  \"duration_seconds\": 312,\n  \"speakers_detected\": 2,\n  \"transcription_accuracy\": 0.94,\n  \"segments\": [\n    {\n      \"id\": 1,\n      \"speaker\": \"Agent\",\n      \"start\": 0.0,\n      \"end\": 4.2,\n      \"text\": \"Thank you for calling support. How can I help you today?\",\n      \"confidence\": 0.98\n    },\n    {\n      \"id\": 2,\n      \"speaker\": \"Customer\",\n      \"start\": 4.5,\n      \"end\": 11.8,\n      \"text\": \"I need to cancel my subscription. The pricing just doesn't work for us.\",\n      \"confidence\": 0.96\n    },\n    {\n      \"id\": 3,\n      \"speaker\": \"Agent\",\n      \"start\": 12.1,\n      \"end\": 18.4,\n      \"text\": \"I'm sorry to hear that. Can I ask what specifically isn't working for you?\",\n      \"confidence\": 0.97\n    }\n  ],\n  \"summary\": \"Customer called to cancel subscription due to pricing concerns. Agent attempted retention. No resolution reached \u2014 escalation recommended.\",\n  \"pii_redacted\": true,\n  \"created_at\": \"2025-04-15T14:32:00Z\",\n  \"completed_at\": \"2025-04-15T14:33:47Z\"\n}\n</code></pre>"},{"location":"api-reference/transcription/#response-fields","title":"Response fields","text":"Field Type Description <code>job_id</code> string The job identifier. <code>status</code> string Always <code>completed</code> when transcript is available. <code>language</code> string BCP-47 language tag detected or specified at upload. <code>duration_seconds</code> integer Total audio duration in seconds. <code>speakers_detected</code> integer Number of distinct speakers identified by diarization. <code>transcription_accuracy</code> float Aggregate confidence score across all segments (0.0\u20131.0). <code>segments</code> array Ordered list of transcript segments. See Segment object. <code>summary</code> string AI-generated conversation summary. Present only when <code>pipeline=full-analysis</code>. <code>pii_redacted</code> boolean Whether PII has been redacted in this response. <code>created_at</code> string ISO 8601 timestamp when the job was created. <code>completed_at</code> string ISO 8601 timestamp when processing finished."},{"location":"api-reference/transcription/#segment-object","title":"Segment object","text":"<p>Each object in the <code>segments</code> array represents a continuous speech segment attributed to a single speaker.</p> Field Type Description <code>id</code> integer Segment index, starting at 1. <code>speaker</code> string Speaker label. Values: <code>Agent</code>, <code>Customer</code>, or <code>Speaker N</code> for unlabeled speakers. <code>start</code> float Segment start time in seconds from the beginning of the audio. <code>end</code> float Segment end time in seconds. <code>text</code> string Transcribed text for this segment. PII is redacted if <code>pii_redacted</code> is <code>true</code>. <code>confidence</code> float Model confidence score for this segment (0.0\u20131.0). Segments below 0.75 may contain errors."},{"location":"api-reference/transcription/#export-formats","title":"Export formats","text":"Format Use case <code>json</code> Programmatic processing, integration with downstream systems <code>srt</code> Subtitle files for video players <code>vtt</code> Web Video Text Tracks, for use in HTML5 <code>&lt;video&gt;</code> elements <code>txt</code> Plain text, no timestamps or speaker labels"},{"location":"api-reference/transcription/#response-codes","title":"Response codes","text":"Code Meaning <code>200 OK</code> Transcript retrieved successfully. <code>401 Unauthorized</code> Invalid or missing API key. <code>403 Forbidden</code> Insufficient permissions. Check your key's scope. <code>404 Not Found</code> Job ID not found, or job is still processing. <code>429 Too Many Requests</code> Rate limit exceeded. <p>Speaker label accuracy</p> <p>Speaker labels (<code>Agent</code>, <code>Customer</code>) are assigned automatically based on conversation patterns. For calls with more than two speakers, labels appear as <code>Speaker 1</code>, <code>Speaker 2</code>, etc. Speaker enrollment for higher accuracy is planned for v1.2.</p>"},{"location":"api-reference/transcription/#related","title":"Related","text":"<ul> <li>Audio Ingestion API \u2014 Upload audio and get a job ID</li> <li>Sentiment Analysis API \u2014 Retrieve sentiment data for the same job</li> <li>Upload &amp; Analyze tutorial</li> <li>Error codes</li> </ul>"},{"location":"architecture/data-flow/","title":"Data Flow","text":"<p>This page traces the complete journey of an audio file through the NovaPulse platform, from upload to the moment results are available via the API.</p>"},{"location":"architecture/data-flow/#end-to-end-flow","title":"End-to-end flow","text":"<pre><code>sequenceDiagram\n  participant Client as Client\n  participant Gateway as API Gateway\n  participant Store as Audio Store\n  participant Pipeline as AI Pipeline\n  participant DB as Storage Layer\n  participant Webhook as Webhook Dispatcher\n\n  Client-&gt;&gt;Gateway: POST /v1/audio/ingest\n  Gateway-&gt;&gt;Gateway: Authenticate and validate\n  Gateway-&gt;&gt;Store: Store audio file (encrypted)\n  Gateway--&gt;&gt;Client: 202 Accepted, job_id returned\n\n  Store-&gt;&gt;Pipeline: Trigger processing job\n  Pipeline-&gt;&gt;Pipeline: ASR transcription\n  Pipeline-&gt;&gt;Pipeline: Speaker diarization\n  Pipeline-&gt;&gt;Pipeline: NER and PII detection\n  Pipeline-&gt;&gt;Pipeline: Sentiment analysis\n  Pipeline-&gt;&gt;Pipeline: Behavioral metrics\n  Pipeline-&gt;&gt;Pipeline: AI summarization\n  Pipeline-&gt;&gt;Pipeline: Vector embedding\n\n  Pipeline-&gt;&gt;DB: Write transcript, insights, vectors\n  DB--&gt;&gt;Pipeline: Confirmed\n\n  Pipeline-&gt;&gt;Webhook: Notify job completed\n  Webhook-&gt;&gt;Client: POST webhook_url (job.completed)\n\n  Client-&gt;&gt;Gateway: GET /v1/transcriptions/job_id\n  Gateway-&gt;&gt;DB: Fetch transcript\n  DB--&gt;&gt;Gateway: Transcript data\n  Gateway--&gt;&gt;Client: 200 OK, transcript response</code></pre>"},{"location":"architecture/data-flow/#step-by-step-breakdown","title":"Step-by-step breakdown","text":""},{"location":"architecture/data-flow/#1-upload-and-validation","title":"1. Upload and validation","text":"<p>The client sends a <code>multipart/form-data</code> POST request to <code>/v1/audio/ingest</code>. The API Gateway authenticates the request, validates the file format and size, and returns a <code>202 Accepted</code> response with a <code>job_id</code> immediately. The client does not wait for processing to complete.</p>"},{"location":"architecture/data-flow/#2-audio-storage","title":"2. Audio storage","text":"<p>The validated audio file is stored in encrypted object storage. Storage is isolated per tenant. The file is assigned a unique internal identifier linked to the job record.</p>"},{"location":"architecture/data-flow/#3-asr-transcription","title":"3. ASR transcription","text":"<p>The transcription engine processes the audio and produces a raw text transcript with word-level timestamps and confidence scores. This is the most computationally intensive step. For a 5-minute call, transcription typically completes in under 30 seconds.</p>"},{"location":"architecture/data-flow/#4-speaker-diarization","title":"4. Speaker diarization","text":"<p>The diarization engine segments the transcript by speaker, assigning consistent labels throughout the call. Diarization runs on the audio waveform in parallel with transcription and is merged with the transcript output at this step.</p>"},{"location":"architecture/data-flow/#5-ner-and-pii-detection","title":"5. NER and PII detection","text":"<p>The named entity recognition model scans the transcript for personally identifiable information: names, phone numbers, email addresses, account numbers, and dates of birth. Detected PII is redacted before the transcript is written to storage.</p>"},{"location":"architecture/data-flow/#6-sentiment-analysis","title":"6. Sentiment analysis","text":"<p>The sentiment model scores each speaker-attributed segment on a scale from -1.0 to +1.0. An overall call sentiment score and trend classification are calculated from the segment scores.</p>"},{"location":"architecture/data-flow/#7-behavioral-metrics","title":"7. Behavioral metrics","text":"<p>Conversation dynamics are calculated from the diarized transcript: silence ratio, interruption count, talk ratio per speaker, and average response time between speakers.</p>"},{"location":"architecture/data-flow/#8-ai-summarization","title":"8. AI summarization","text":"<p>A large language model generates a concise summary of the conversation, grounded in the actual transcript text. The summary covers the reason for the call, key topics, outcome, and recommended follow-up actions.</p>"},{"location":"architecture/data-flow/#9-vector-embedding","title":"9. Vector embedding","text":"<p>Each transcript segment is converted to a dense vector embedding and written to the vector index. This step runs in parallel with summarization and enables semantic search across all stored conversations after the job completes.</p>"},{"location":"architecture/data-flow/#10-results-available","title":"10. Results available","text":"<p>All results are written to the storage layer. The job status is updated to <code>completed</code>. If a <code>webhook_url</code> was provided at upload, the Webhook Dispatcher sends a <code>job.completed</code> notification to the client.</p> <p>The client can now retrieve results via:</p> <ul> <li><code>GET /v1/transcriptions/{job_id}</code></li> <li><code>GET /v1/insights/{job_id}</code></li> </ul>"},{"location":"architecture/data-flow/#processing-times","title":"Processing times","text":"<p>Processing time depends on audio duration and current queue depth. Typical times for the <code>full-analysis</code> pipeline:</p> Audio duration Typical processing time Under 5 minutes 30 to 90 seconds 5 to 15 minutes 90 seconds to 4 minutes 15 to 60 minutes 4 to 15 minutes Over 60 minutes 15 minutes or more <p>Note</p> <p>These are typical times under normal load. During high-demand periods, processing times may be longer. Use webhooks rather than polling to avoid unnecessary load on your integration.</p>"},{"location":"architecture/data-flow/#asynchronous-design","title":"Asynchronous design","text":"<p>NovaPulse uses an asynchronous processing model by design. The upload endpoint returns immediately so that your application is never blocked waiting for a long-running AI pipeline to complete. This makes it straightforward to process large batches of audio files in parallel without managing timeouts on long-running HTTP connections.</p>"},{"location":"architecture/data-flow/#related","title":"Related","text":"<ul> <li>System Overview</li> <li>Security and Compliance</li> <li>Audio Ingestion API</li> <li>NLP Capabilities</li> </ul>"},{"location":"architecture/security/","title":"Security and Compliance","text":"<p>NovaPulse is designed with data protection as a foundational requirement, not an afterthought. This page describes the security controls, PII handling, and compliance posture of the platform.</p>"},{"location":"architecture/security/#encryption","title":"Encryption","text":"Data state Standard Details In transit TLS 1.3 All API communication is encrypted. HTTP requests are rejected. At rest AES-256 Audio files, transcripts, vectors, and insights are encrypted at rest. Keys Cloud KMS Encryption keys are managed by a cloud key management service with automatic rotation."},{"location":"architecture/security/#pii-detection-and-redaction","title":"PII detection and redaction","text":"<p>NovaPulse automatically detects and redacts personally identifiable information from all transcripts before they are stored or returned via the API.</p>"},{"location":"architecture/security/#what-is-detected","title":"What is detected","text":"Entity type Examples Full names \"My name is Sarah Johnson\" Phone numbers \"+34 612 345 678\", \"(555) 123-4567\" Email addresses \"contact@example.com\" Account numbers \"Account 4821-XXXX\" Dates of birth \"born on March 12, 1985\" Physical addresses \"123 Main Street, Madrid\""},{"location":"architecture/security/#how-redaction-works","title":"How redaction works","text":"<p>Detected PII is replaced with a typed placeholder in the stored transcript:</p> <pre><code>Original:  \"My name is Sarah Johnson, my account number is 48219821.\"\nRedacted:  \"My name is [NAME], my account number is [ACCOUNT_NUMBER].\"\n</code></pre> <p>Redaction is applied before the transcript is written to storage. By default, all API responses return the redacted version.</p>"},{"location":"architecture/security/#accessing-unredacted-transcripts","title":"Accessing unredacted transcripts","text":"<p>Unredacted transcripts are available to API keys with the <code>pii:read</code> permission scope. This scope must be explicitly granted in the dashboard and should be restricted to authorized compliance workflows only.</p> <pre><code>curl \"https://api.novapulse.io/v1/transcriptions/job_id?redact_pii=false\" \\\n  -H \"Authorization: Bearer nvp_live_KEY_WITH_PII_READ_SCOPE\"\n</code></pre> <p>Handle unredacted data carefully</p> <p>Unredacted transcripts contain personal data. Ensure your integration stores and transmits this data in compliance with applicable data protection regulations.</p>"},{"location":"architecture/security/#multi-tenant-isolation","title":"Multi-tenant isolation","text":"<p>NovaPulse serves multiple customers from shared infrastructure. Data isolation is enforced at every layer:</p> <ul> <li>API layer: API keys are scoped to a single tenant. No cross-tenant   request path exists.</li> <li>Storage layer: Row-level security ensures that database queries   only return records belonging to the authenticated tenant.</li> <li>Processing layer: Jobs are processed in isolated compute contexts.   No data is shared between tenants during processing.</li> </ul>"},{"location":"architecture/security/#data-retention","title":"Data retention","text":"Data type Default retention Configurable Audio files 90 days Yes, per account Transcripts 90 days Yes, per account Insights and vectors 90 days Yes, per account Job metadata 12 months No <p>After the retention period, data is permanently deleted and cannot be recovered. Configure retention periods in the dashboard under Settings &gt; Data Retention.</p>"},{"location":"architecture/security/#gdpr-compliance","title":"GDPR compliance","text":"<p>NovaPulse is designed to support GDPR compliance for customers processing personal data of EU residents.</p> Requirement NovaPulse implementation Lawful basis for processing Customers are responsible for establishing lawful basis. NovaPulse processes data on behalf of the customer as a data processor. Data processing agreement (DPA) Available on request for all paid plans. Right to erasure Supported via the job deletion API. Deleting a job permanently removes all associated audio, transcript, and insight data. Data minimization PII redaction is enabled by default. Unredacted access requires explicit permission. Data location Data is stored in the EU region by default. US and APAC regions available on Enterprise plans. Sub-processors A current list of NovaPulse sub-processors is available on request."},{"location":"architecture/security/#authentication-security","title":"Authentication security","text":"<ul> <li>API keys use a prefixed format (<code>nvp_live_</code> or <code>nvp_test_</code>) to prevent   accidental use of production keys in test environments.</li> <li>Keys are shown only once at creation. NovaPulse does not store the   full key value after creation.</li> <li>Keys can be revoked instantly from the dashboard.</li> <li>All API key usage is logged with timestamp, endpoint, and IP address.</li> </ul>"},{"location":"architecture/security/#responsible-disclosure","title":"Responsible disclosure","text":"<p>To report a security vulnerability in NovaPulse, contact security@novapulse.io with a description of the issue. Do not disclose vulnerabilities publicly before they have been addressed.</p>"},{"location":"architecture/security/#related","title":"Related","text":"<ul> <li>System Overview</li> <li>Data Flow</li> <li>Authentication</li> <li>Error Codes</li> </ul>"},{"location":"architecture/system-overview/","title":"System Overview","text":"<p>NovaPulse is built on a cloud-native, multi-tenant architecture designed for reliability, scalability, and data isolation. This page describes the platform's core components and how they work together.</p>"},{"location":"architecture/system-overview/#architecture-diagram","title":"Architecture diagram","text":"<pre><code>flowchart TB\n  subgraph Clients[\"Clients\"]\n    A1[Developer API]\n    A2[SaaS Dashboard]\n    A3[Webhook Consumer]\n  end\n\n  subgraph Ingestion[\"Ingestion Layer\"]\n    B1[REST API Gateway]\n    B2[Audio Validation]\n    B3[Secure Audio Store]\n  end\n\n  subgraph Processing[\"AI Processing Layer\"]\n    C1[ASR Engine]\n    C2[Speaker Diarization]\n    C3[NER &amp; PII Detection]\n    C4[Sentiment Analysis]\n    C5[AI Summarization]\n    C6[Vector Embedding]\n  end\n\n  subgraph Storage[\"Storage Layer\"]\n    D1[Transcript Store]\n    D2[Vector Index]\n    D3[Insights Store]\n  end\n\n  subgraph Delivery[\"Delivery Layer\"]\n    E1[Results API]\n    E2[Webhook Dispatcher]\n    E3[Analytics Dashboard]\n  end\n\n  Clients --&gt; B1\n  B1 --&gt; B2 --&gt; B3\n  B3 --&gt; C1 --&gt; C2 --&gt; C3 --&gt; C4 --&gt; C5\n  C2 --&gt; C6\n  C6 --&gt; D2\n  C3 --&gt; D1\n  C4 --&gt; D3\n  C5 --&gt; D3\n  D1 &amp; D2 &amp; D3 --&gt; E1\n  E1 --&gt; Clients\n  E2 --&gt; A3\n\n  style Clients fill:#0f2027,color:#ffffff,stroke:#0d6e6e\n  style Ingestion fill:#0d3a3a,color:#ffffff,stroke:#0d6e6e\n  style Processing fill:#083d2a,color:#ffffff,stroke:#065f46\n  style Storage fill:#0d3a3a,color:#ffffff,stroke:#0d6e6e\n  style Delivery fill:#0a2a2a,color:#ffffff,stroke:#0d6e6e</code></pre>"},{"location":"architecture/system-overview/#core-components","title":"Core components","text":""},{"location":"architecture/system-overview/#api-gateway","title":"API Gateway","text":"<p>The API Gateway is the single entry point for all client requests. It handles:</p> <ul> <li>Authentication and API key validation</li> <li>Request routing to the appropriate internal service</li> <li>Rate limiting enforcement per API key</li> <li>TLS termination</li> </ul> <p>All traffic to NovaPulse passes through the API Gateway. No internal services are directly accessible from outside the platform.</p>"},{"location":"architecture/system-overview/#audio-validation-and-storage","title":"Audio Validation and Storage","text":"<p>When an audio file is uploaded, it passes through a validation step that checks file format, encoding, size, and duration before storage. Valid files are stored in encrypted object storage with a unique identifier tied to the processing job.</p> <p>Audio files are retained for 90 days by default and are never shared across tenant boundaries.</p>"},{"location":"architecture/system-overview/#ai-processing-pipeline","title":"AI Processing Pipeline","text":"<p>The processing pipeline runs as a sequence of independent services. Each service receives the output of the previous step and adds its own structured data to the job record. See Data Flow for the step-by-step sequence and NLP Capabilities for details on each AI model.</p>"},{"location":"architecture/system-overview/#vector-index","title":"Vector Index","text":"<p>After transcription and diarization, each transcript segment is converted to a dense vector embedding and stored in the vector index. This enables semantic search across all conversation data. See Vector Search for how this works.</p>"},{"location":"architecture/system-overview/#results-api","title":"Results API","text":"<p>The Results API serves completed job data to clients: transcripts, sentiment analysis, behavioral metrics, and AI summaries. All results are served from the storage layer; the AI processing pipeline does not run again on read requests.</p>"},{"location":"architecture/system-overview/#webhook-dispatcher","title":"Webhook Dispatcher","text":"<p>When a job completes, the Webhook Dispatcher sends a POST notification to the client's configured <code>webhook_url</code>. It handles retries with exponential backoff and logs delivery status per job.</p>"},{"location":"architecture/system-overview/#multi-tenancy-and-isolation","title":"Multi-tenancy and isolation","text":"<p>NovaPulse is a multi-tenant platform. All data is isolated at the storage layer using row-level security: no tenant can access another tenant's audio, transcripts, or insights under any circumstances.</p> <p>API keys are scoped to a single tenant. There is no cross-tenant API access path.</p>"},{"location":"architecture/system-overview/#cloud-infrastructure","title":"Cloud infrastructure","text":"<p>NovaPulse is deployed exclusively on cloud infrastructure. There is no on-premise option in v1.0. The platform is designed for horizontal scalability: processing nodes scale automatically based on job queue depth.</p>"},{"location":"architecture/system-overview/#external-integrations","title":"External integrations","text":"Service type Purpose Integration method Third-party ASR provider Fallback transcription for edge cases REST API LLM provider AI summarization and recommendations Inference API CRM systems Ticket and call record linking via metadata Metadata field mapping VoIP and WebRTC platforms Real-time audio stream ingestion (Phase 2) WebRTC stream"},{"location":"architecture/system-overview/#related","title":"Related","text":"<ul> <li>Data Flow \u2014 Step-by-step data flow through the pipeline</li> <li>Security and Compliance \u2014 Encryption, PII, and GDPR</li> <li>NLP Capabilities</li> <li>Vector Search</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>These guides explain the core ideas behind NovaPulse. Read them to understand how the platform works before integrating the API.</p>"},{"location":"concepts/#in-this-section","title":"In this section","text":"Guide Description Platform Overview What NovaPulse is, its three phases, and who it's for Vector Search How NovaPulse uses semantic search across conversation data RAG Pipeline How retrieval-augmented generation powers AI-driven insights NLP Capabilities Transcription, diarization, sentiment, NER, and summarization"},{"location":"concepts/#new-to-novapulse","title":"New to NovaPulse?","text":"<p>Start with the Platform Overview to understand what NovaPulse does and how it's structured. Then follow the Getting Started tutorial to make your first API call.</p>"},{"location":"concepts/nlp-capabilities/","title":"NLP Capabilities","text":"<p>NovaPulse uses a pipeline of natural language processing (NLP) models to transform raw audio into structured data. Each capability runs automatically when you process audio with <code>pipeline=full-analysis</code>.</p>"},{"location":"concepts/nlp-capabilities/#automatic-speech-recognition-asr","title":"Automatic Speech Recognition (ASR)","text":"<p>ASR, also called transcription, converts spoken audio into text. NovaPulse's transcription engine is optimized for telephone-quality audio and multi-speaker conversations.</p> <p>Performance targets:</p> Condition Accuracy Clear audio, single speaker \u226596% Multi-speaker, standard quality \u226590% Background noise or heavy accent \u226582% <p>Accuracy is measured as word error rate (WER) against human-verified transcripts across a diverse test set.</p>"},{"location":"concepts/nlp-capabilities/#speaker-diarization","title":"Speaker Diarization","text":"<p>Diarization answers the question: who spoke when?</p> <p>NovaPulse's diarization engine segments the audio by speaker and assigns a consistent label to each speaker throughout the call : <code>Agent</code>, <code>Customer</code>, or <code>Speaker N</code> for calls with more than two participants.</p> <p>Diarization runs in parallel with transcription so that every transcript segment is speaker-attributed from the start.</p> <p>Note</p> <p>Speaker labels are assigned based on conversation patterns, not voice enrollment. For calls with very similar voice profiles, label accuracy may be lower. Voice enrollment for improved accuracy is planned for v1.2.</p>"},{"location":"concepts/nlp-capabilities/#sentiment-analysis","title":"Sentiment Analysis","text":"<p>NovaPulse scores each transcript segment on a continuous sentiment scale from -1.0 (strongly negative) to +1.0 (strongly positive), and produces an overall call sentiment classification.</p> <p>The sentiment model is trained on customer service conversation data and is optimized for detecting:</p> <ul> <li>Customer frustration and dissatisfaction</li> <li>Positive engagement and satisfaction signals</li> <li>Sentiment shifts during the conversation (trend detection)</li> <li>High-risk moments requiring supervisor intervention</li> </ul> <p>See Sentiment Analysis API for the full response schema.</p>"},{"location":"concepts/nlp-capabilities/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<p>NER identifies and classifies named entities in transcript text. NovaPulse uses NER for two purposes:</p> <p>PII detection: Identifies names, phone numbers, email addresses, account numbers, dates of birth, and other personally identifiable information so they can be automatically redacted before storage.</p> <p>Structured data extraction: Extracts business-relevant entities : product names, company names, locations, and dates, to enable downstream filtering and search.</p>"},{"location":"concepts/nlp-capabilities/#ai-summarization","title":"AI Summarization","text":"<p>NovaPulse generates a concise summary of each conversation automatically, covering:</p> <ul> <li>The primary reason for the call</li> <li>Key topics discussed</li> <li>Outcome or resolution status</li> <li>Recommended follow-up actions</li> </ul> <p>Summaries are generated using a large language model grounded in the actual transcript text not hallucinated. The transcript content is used as the context, ensuring the summary accurately reflects what was said.</p>"},{"location":"concepts/nlp-capabilities/#behavioral-analytics","title":"Behavioral Analytics","text":"<p>Beyond language, NovaPulse measures conversation dynamics:</p> Metric Description Silence ratio Proportion of the call spent in silence, high values indicate hold time or confusion Interruption count Number of times either speaker interrupted the other Talk ratio Proportion of speaking time per speaker, it helps identify agent-dominated calls Average response time Mean time between speakers, long gaps may indicate confusion or hesitation"},{"location":"concepts/nlp-capabilities/#processing-pipeline","title":"Processing pipeline","text":"<p>When you upload audio with <code>pipeline=full-analysis</code>, NovaPulse runs these steps in sequence:</p> <pre><code>flowchart TD\n  A[Audio upload] --&gt; B[Audio validation &amp; format conversion]\n  B --&gt; C[ASR Transcription]\n  C --&gt; D[Speaker Diarization]\n  D --&gt; E[NER &amp; PII Detection]\n  E --&gt; F[Sentiment Analysis]\n  F --&gt; G[Behavioral Metrics]\n  G --&gt; H[AI Summarization]\n  H --&gt; I[Results stored &amp; available via API]\n\n  style A fill:#0f2027,color:#ffffff,stroke:#0d6e6e\n  style B fill:#0d6e6e,color:#ffffff,stroke:#0d6e6e\n  style C fill:#0d6e6e,color:#ffffff,stroke:#0d6e6e\n  style D fill:#0d6e6e,color:#ffffff,stroke:#0d6e6e\n  style E fill:#14b8a6,color:#0f2027,stroke:#14b8a6\n  style F fill:#14b8a6,color:#0f2027,stroke:#14b8a6\n  style G fill:#14b8a6,color:#0f2027,stroke:#14b8a6\n  style H fill:#065f46,color:#ffffff,stroke:#065f46\n  style I fill:#065f46,color:#ffffff,stroke:#065f46</code></pre>"},{"location":"concepts/nlp-capabilities/#related","title":"Related","text":"<ul> <li>Platform Overview</li> <li>Vector Search \u2014 Semantic search across transcripts</li> <li>RAG Pipeline \u2014 AI-driven question answering</li> <li>Sentiment Analysis API</li> <li>Transcription API</li> </ul>"},{"location":"concepts/platform-overview/","title":"Platform Overview","text":"<p>NovaPulse is an AI Voice Agentic Infrastructure Platform that transforms raw audio conversations into structured, actionable intelligence. It is designed to integrate into enterprise business processes as foundational infrastructure and not as a point solution.</p>"},{"location":"concepts/platform-overview/#the-problem-novapulse-solves","title":"The problem NovaPulse solves","text":"<p>Organizations conduct thousands of voice conversations every day such as customer support calls, sales negotiations, HR interviews, compliance assessments. Each conversation carries significant strategic value: customer sentiment, compliance signals, sales opportunities, employee engagement indicators.</p> <p>Despite this, most organizations treat voice as transient. Recordings sit unused. Manual review is slow, inconsistent, and doesn't scale. Insights arrive days or weeks after the conversation, too late to act on.</p> <p>NovaPulse addresses this by treating voice as a structured, searchable, analyzable data source in real time.</p>"},{"location":"concepts/platform-overview/#three-phases-of-intelligence","title":"Three phases of intelligence","text":"<p>NovaPulse delivers its capabilities in three progressive phases:</p> <pre><code>flowchart LR\n  A[Phase 1 Structured Intelligence] --&gt; B[Phase 2 Real-Time Analytics] --&gt; C[Phase 3 Autonomous Voice Agents]\n\n  style A fill:#0d6e6e,stroke:#5eead4,stroke-width:2px,color:#ffffff\n  style B fill:#14b8a6,stroke:#5eead4,stroke-width:2px,color:#ffffff\n  style C fill:#065f46,stroke:#5eead4,stroke-width:2px,color:#ffffff</code></pre>"},{"location":"concepts/platform-overview/#phase-1-structured-intelligence-ga","title":"Phase 1: Structured intelligence (GA)","text":"<p>Upload recorded audio and receive structured insights: speaker-attributed transcripts, sentiment analysis, AI-generated summaries, and PII redaction. Delivered through a REST API and SaaS dashboard.</p> <p>Who it's for: customer support teams, compliance departments, sales enablement, HR, and healthcare providers who need to analyze recorded conversations at scale.</p>"},{"location":"concepts/platform-overview/#phase-2-real-time-analytics-beta","title":"Phase 2: Real-time analytics (Beta)","text":"<p>Integrate with live audio streams to receive transcription, sentiment scoring, and behavioral alerts during an ongoing conversation with latency under 2 seconds. Supervisors can monitor calls live and intervene in real time.</p> <p>Who it's for: call center supervisors, quality assurance teams, and compliance officers who need live visibility into conversations.</p>"},{"location":"concepts/platform-overview/#phase-3-autonomous-voice-agents-planned","title":"Phase 3: Autonomous voice agents (Planned)","text":"<p>Deploy AI voice agents capable of managing full conversations independently: understanding context, responding dynamically, executing tasks, and escalating to humans when required. Built on accumulated datasets from Phases 1 and 2.</p> <p>Who it's for: organizations looking to automate high-volume, structured voice interactions in support, sales, or HR workflows.</p>"},{"location":"concepts/platform-overview/#target-industries","title":"Target industries","text":"Industry Primary use case Customer support Call quality monitoring, sentiment-based escalation, agent coaching Sales Conversation intelligence, objection detection, deal risk signals Human resources Interview analysis, candidate sentiment, compliance documentation Healthcare Patient interaction documentation, compliance audit trails Financial services Regulatory compliance, call archiving, risk detection"},{"location":"concepts/platform-overview/#platform-architecture","title":"Platform architecture","text":"<p>NovaPulse is built on a cloud-native, multi-tenant architecture with four core layers:</p> <ul> <li>Ingestion layer - Accepts audio via REST API or real-time stream</li> <li>AI processing layer - Transcription, diarization, NLP, and summarization</li> <li>Storage layer - Encrypted audio storage and vector-indexed transcript store</li> <li>Delivery layer - REST API, webhooks, and dashboard</li> </ul> <p>See System Architecture for a full technical breakdown.</p>"},{"location":"concepts/platform-overview/#key-design-principles","title":"Key design principles","text":"<p>Privacy by design. PII detection and redaction run automatically on every transcript. Data is encrypted in transit and at rest. GDPR-ready by default.</p> <p>API-first. Every NovaPulse capability is accessible via REST API. The dashboard is built on the same API available to developers.</p> <p>Phased value delivery. Each phase delivers immediate business value independently. Organizations don't need to wait for Phase 3 to benefit from Phase 1.</p>"},{"location":"concepts/platform-overview/#related","title":"Related","text":"<ul> <li>Getting Started tutorial</li> <li>Vector Search \u2014 How NovaPulse enables semantic search</li> <li>NLP Capabilities \u2014 The AI models powering NovaPulse</li> <li>API Reference</li> </ul>"},{"location":"concepts/rag-pipeline/","title":"RAG Pipeline","text":"<p>Retrieval-Augmented Generation (RAG) is the architecture that powers NovaPulse's AI-driven insights. It combines the recall capability of vector search with the reasoning capability of a large language model (LLM) to answer natural language questions about your conversation data.</p>"},{"location":"concepts/rag-pipeline/#why-rag-not-a-standalone-llm","title":"Why RAG, not a standalone LLM","text":"<p>A standalone LLM cannot answer questions about your data:</p> <ul> <li>It was trained on public data up to a cutoff date</li> <li>It has no access to your call transcripts</li> <li>It will generate plausible-sounding but wrong answers (hallucinations)</li> </ul> <p>RAG solves this by giving the LLM real, current context from your own data before it generates a response.</p>"},{"location":"concepts/rag-pipeline/#the-three-step-rag-flow","title":"The three-step RAG flow","text":"<pre><code>flowchart LR\n  A[User query] --&gt; B[Embed query into vector]\n  B --&gt; C[Search transcript vector store]\n  C --&gt; D[Retrieve top-k relevant segments]\n  D --&gt; E[Augment LLM prompt with context]\n  E --&gt; F[LLM generates grounded response]\n  F --&gt; G[Response with source citations]\n\n  style A fill:#0f2027,color:#ffffff,stroke:#0d6e6e\n  style B fill:#0d6e6e,color:#ffffff,stroke:#0d6e6e\n  style C fill:#0d6e6e,color:#ffffff,stroke:#0d6e6e\n  style D fill:#14b8a6,color:#0f2027,stroke:#14b8a6\n  style E fill:#14b8a6,color:#0f2027,stroke:#14b8a6\n  style F fill:#065f46,color:#ffffff,stroke:#065f46\n  style G fill:#065f46,color:#ffffff,stroke:#065f46</code></pre>"},{"location":"concepts/rag-pipeline/#step-1-retrieve","title":"Step 1: Retrieve","text":"<p>The user's question is converted to a vector using NovaPulse's embedding model. A kNN search finds the transcript segments most semantically similar to the question.</p>"},{"location":"concepts/rag-pipeline/#step-2-augment","title":"Step 2: Augment","text":"<p>The retrieved segments are injected into the LLM prompt:</p> <pre><code>Context from call transcripts:\n[Segment 1: CALL-0042, timestamp 4:12] Customer: This charge is completely\nunexpected. I was never told about the $49 monthly fee...\n\n[Segment 2: CALL-0089, timestamp 2:31] Customer: Why is my bill so much\nhigher than last month? Nobody explained the price changes...\n\nBased on the above context, answer: What billing concerns are customers\nraising most frequently this week?\n</code></pre>"},{"location":"concepts/rag-pipeline/#step-3-generate","title":"Step 3: Generate","text":"<p>The LLM generates a response grounded in the actual transcript data, with references to the source calls. NovaPulse includes citation links so supervisors can navigate directly to the relevant conversation.</p>"},{"location":"concepts/rag-pipeline/#novapulses-vector-store","title":"NovaPulse's vector store","text":"<p>NovaPulse stores all transcript segment embeddings in a high-performance vector database indexed for approximate nearest neighbor (ANN) search. This enables sub-second semantic search across millions of transcript segments with configurable recall-latency tradeoffs.</p>"},{"location":"concepts/rag-pipeline/#related","title":"Related","text":"<ul> <li>Vector Search \u2014 Embedding and kNN search explained</li> <li>NLP Capabilities \u2014 How transcripts are generated</li> <li>System Architecture</li> </ul>"},{"location":"concepts/vector-search/","title":"Vector Search in NovaPulse","text":"<p>Traditional keyword search looks for exact word matches. When a supervisor searches call transcripts for \"cancellation\", they will miss transcripts where a customer said \"I want to leave\" or \"thinking about switching\". For voice analytics, this is a critical limitation.</p> <p>NovaPulse uses vector search , also called semantic search, to find content based on meaning rather than exact words.</p>"},{"location":"concepts/vector-search/#how-vector-embeddings-work","title":"How vector embeddings work","text":"<p>When NovaPulse processes a transcript, each segment of speech is passed through an embedding model that converts the text into a dense vector: a list of numbers that encodes the semantic meaning of the text.</p> <pre><code>\"I want to cancel my subscription\"  \u2192  [0.23, -0.87, 0.44, 0.91, ...]\n\"thinking about switching providers\" \u2192  [0.21, -0.89, 0.46, 0.88, ...]  \u2190 similar\n\"great experience today, thank you\"  \u2192  [-0.91, 0.33, -0.67, 0.12, ...] \u2190 different\n</code></pre> <p>Texts with similar meaning produce vectors that are mathematically close together. Distance between vectors is measured using cosine similarity.</p>"},{"location":"concepts/vector-search/#semantic-search-in-novapulse","title":"Semantic search in NovaPulse","text":"<p>When you use the Insights API to search conversations, NovaPulse:</p> <ol> <li>Converts your query into a vector using the same embedding model</li> <li>Performs a k-nearest neighbor (kNN) search across all stored transcript vectors</li> <li>Returns the conversations whose vectors are closest to your query vector</li> <li>Ranks results by semantic similarity, not keyword frequency</li> </ol>"},{"location":"concepts/vector-search/#example","title":"Example","text":"<p>A support manager queries:</p> <pre><code>\"calls where customers expressed frustration about billing\"\n</code></pre> <p>NovaPulse returns relevant calls even if they contain none of those exact words, including transcripts where customers said \"this charge makes no sense\", \"I'm really annoyed about this invoice\", or \"nobody explained the fees to me\".</p>"},{"location":"concepts/vector-search/#hybrid-search","title":"Hybrid search","text":"<p>For queries involving exact terms: product names, error codes, or precise phrases, pure semantic search can underperform. NovaPulse supports hybrid search that combines:</p> Method Strengths BM25 (keyword) Exact term matching, product names, error codes kNN (semantic) Concept matching, synonyms, conversational language Hybrid (RRF) Best results for most real-world queries <p>Results from both methods are merged using Reciprocal Rank Fusion (RRF), which combines rankings without requiring score normalization.</p>"},{"location":"concepts/vector-search/#related-concepts","title":"Related concepts","text":"<ul> <li>RAG Pipeline \u2014 How NovaPulse uses retrieval for AI answers</li> <li>NLP Capabilities \u2014 Transcription, diarization, sentiment</li> <li>Glossary \u2014 Definitions of all AI terms</li> </ul>"},{"location":"kibana-lab/","title":"Kibana Lab","text":"<p>This section documents hands-on exploration of Kibana using NovaPulse call analytics data. Every page was written by actually building the things described: indexing data, running queries, and building visualizations. Nothing here comes from reading documentation alone.</p>"},{"location":"kibana-lab/#what-this-lab-covers","title":"What this lab covers","text":"Page What you will find Index NovaPulse data Create an Elasticsearch index mapping, index 12 sample call records using Dev Tools, and verify the data Explore with Discover Use KQL to filter by sentiment, team, resolution status, and score ranges Build a Lens dashboard Build a 3-panel call analytics dashboard with a sentiment pie chart, daily volume by team, and agent performance chart"},{"location":"kibana-lab/#environment","title":"Environment","text":"<ul> <li>Platform: Elastic Cloud (free trial, GCP europe-west1)</li> <li>Interface: Kibana Serverless</li> <li>Data: NovaPulse call analytics - 12 sample records across 5 agents, 3 teams, 3 regions</li> <li>Date range: April 1 to April 4, 2025</li> <li>Tools used: Dev Tools console, Kibana Discover, Kibana Lens</li> </ul>"},{"location":"kibana-lab/#why-novapulse-data","title":"Why NovaPulse data","text":"<p>Rather than using Elastic's generic sample datasets, this lab indexes NovaPulse-shaped data: real fields from the NovaPulse API response schema including <code>overall_sentiment</code>, <code>sentiment_score</code>, <code>agent_id</code>, <code>team</code>, <code>silence_ratio</code>, and <code>interruption_count</code>. This makes the visualizations directly relevant to the platform documented in this site, and demonstrates end-to-end thinking from API design through to data analysis.</p> <p>About this lab</p> <p>These pages document real exploration, including problems encountered and how they were solved. The goal is to show hands-on technical curiosity and the ability to figure things out independently.</p>"},{"location":"kibana-lab/kibana-discover/","title":"Explore with Discover","text":"<p>Kibana Discover is the primary tool for exploring raw Elasticsearch data. It provides a document table, field statistics panel, a time-series histogram, and a KQL search bar. This page documents exploration of NovaPulse call analytics data using Discover.</p> <p>Official reference: Discover</p>"},{"location":"kibana-lab/kibana-discover/#open-discover","title":"Open Discover","text":"<ol> <li>Click Discover in the left sidebar</li> <li>Select NovaPulse Calls from the data view selector (top left)</li> <li>Set the time range to April 1 to April 5, 2025 using the date picker</li> </ol> <p>You should see 12 documents in the results table and a histogram showing call distribution across the four-day period.</p>"},{"location":"kibana-lab/kibana-discover/#the-discover-interface","title":"The Discover interface","text":"Component Purpose Search bar KQL query input, filters documents in real time Time range picker Restricts results to a specific time window Field list (left panel) All index fields. Click any field to see a value distribution Histogram Document count over time, updates instantly with every filter Document table Individual records. Expand any row to see all fields"},{"location":"kibana-lab/kibana-discover/#kql-query-examples","title":"KQL query examples","text":"<p>KQL (Kibana Query Language) is a filter language built into Kibana. It filters which documents appear in the results but does not aggregate or sort data. As you type, KQL suggests field names and valid values from the index automatically.</p> <p>Official reference: KQL</p>"},{"location":"kibana-lab/kibana-discover/#filter-by-sentiment","title":"Filter by sentiment","text":"<pre><code>overall_sentiment : \"negative\"\n</code></pre> <p>Returns 5 documents. All calls with negative sentiment classification.</p>"},{"location":"kibana-lab/kibana-discover/#filter-by-team-and-sentiment","title":"Filter by team and sentiment","text":"<pre><code>team : \"billing\" AND overall_sentiment : \"negative\"\n</code></pre> <p>Returns billing team calls with negative sentiment only. Useful for identifying the team with the most difficult call volume.</p>"},{"location":"kibana-lab/kibana-discover/#filter-by-numeric-range","title":"Filter by numeric range","text":"<pre><code>sentiment_score &lt; -0.5\n</code></pre> <p>Returns calls with strongly negative sentiment scores. More precise than the classification label and useful for escalation prioritization.</p>"},{"location":"kibana-lab/kibana-discover/#filter-unresolved-negative-calls","title":"Filter unresolved negative calls","text":"<pre><code>resolved : false AND overall_sentiment : \"negative\"\n</code></pre> <p>Returns the highest-priority calls for review: unresolved and negative. This is a query a supervisor would run every morning.</p>"},{"location":"kibana-lab/kibana-discover/#filter-long-unresolved-calls","title":"Filter long unresolved calls","text":"<pre><code>duration_seconds &gt; 300 AND resolved : false\n</code></pre> <p>Returns calls over 5 minutes that were not resolved. Long duration combined with no resolution is a strong signal of a difficult interaction.</p>"},{"location":"kibana-lab/kibana-discover/#wildcard-filter","title":"Wildcard filter","text":"<pre><code>agent_id : AGT-1*\n</code></pre> <p>Returns all calls handled by agents whose ID starts with AGT-1, covering AGT-12 and AGT-19.</p>"},{"location":"kibana-lab/kibana-discover/#analyzing-field-distributions","title":"Analyzing field distributions","text":"<p>Click any field name in the left panel to see a quick value breakdown without writing a query. This is useful for understanding data shape before building visualizations.</p> <p>Try these:</p> <ul> <li>Click overall_sentiment: shows the count split across positive,   neutral, and negative</li> <li>Click team: shows call volume per team. Billing has the most calls</li> <li>Click agent_id: shows which agents handled the most calls</li> <li>Click sentiment_trend: shows the distribution of improving, stable,   and declining trends</li> </ul>"},{"location":"kibana-lab/kibana-discover/#add-columns-to-the-document-table","title":"Add columns to the document table","text":"<p>By default Discover shows only the timestamp and the full document source. Add specific columns to make the table readable:</p> <p>Hover over any field in the left panel and click + to add it as a column. Add: agent_id, team, overall_sentiment, sentiment_score, duration_seconds, resolved.</p> <p>The table now gives a clear per-call overview, similar to a database query result.</p>"},{"location":"kibana-lab/kibana-discover/#save-a-search-for-reuse","title":"Save a search for reuse","text":"<p>After building a useful filter, save it to reuse in dashboards:</p> <ol> <li>Run: <code>resolved : false AND overall_sentiment : \"negative\"</code></li> <li>Click Save in the application menu</li> <li>Name it: <code>Unresolved Negative Calls</code></li> </ol> <p>Saved searches can be added directly to Kibana dashboards as panels.</p>"},{"location":"kibana-lab/kibana-discover/#related","title":"Related","text":"<ul> <li>Index NovaPulse data</li> <li>Build a Lens dashboard</li> <li>KQL reference</li> </ul>"},{"location":"kibana-lab/kibana-index-data/","title":"Index NovaPulse data","text":"<p>Before building visualizations, data needs to be in Elasticsearch. This page covers creating an explicit index mapping for NovaPulse call records and indexing 12 sample documents using the Kibana Dev Tools console.</p> <p>Official reference: Mapping</p>"},{"location":"kibana-lab/kibana-index-data/#what-is-an-index-mapping","title":"What is an index mapping?","text":"<p>An index mapping defines the data type of each field in an Elasticsearch index. Getting field types right before indexing matters because Elasticsearch cannot change a field's type after documents have been indexed.</p> <p>The key decisions for NovaPulse call records:</p> Field Type Reason <code>@timestamp</code> <code>date</code> Enables time-based filtering and histograms in Kibana <code>overall_sentiment</code> <code>keyword</code> Exact-match values: positive, neutral, negative <code>sentiment_score</code> <code>float</code> Continuous value for aggregations and range queries <code>agent_id</code>, <code>team</code>, <code>queue</code> <code>keyword</code> Used for grouping and filtering, must be exact match <code>duration_seconds</code> <code>integer</code> Numeric field for average, min, max aggregations <code>resolved</code> <code>boolean</code> True/false for filtering unresolved calls"},{"location":"kibana-lab/kibana-index-data/#step-1-open-dev-tools","title":"Step 1: Open Dev Tools","text":"<p>In Kibana, click the &lt;/&gt; icon at the bottom of the left sidebar to open Dev Tools. The console has two panels: write requests on the left, view responses on the right. Run a request with Ctrl+Enter or the play button.</p>"},{"location":"kibana-lab/kibana-index-data/#step-2-create-the-index-mapping","title":"Step 2: Create the index mapping","text":"<pre><code>PUT novapulse-calls\n{\n  \"mappings\": {\n    \"properties\": {\n      \"@timestamp\":             { \"type\": \"date\" },\n      \"job_id\":                 { \"type\": \"keyword\" },\n      \"agent_id\":               { \"type\": \"keyword\" },\n      \"team\":                   { \"type\": \"keyword\" },\n      \"call_type\":              { \"type\": \"keyword\" },\n      \"queue\":                  { \"type\": \"keyword\" },\n      \"duration_seconds\":       { \"type\": \"integer\" },\n      \"overall_sentiment\":      { \"type\": \"keyword\" },\n      \"sentiment_score\":        { \"type\": \"float\" },\n      \"sentiment_trend\":        { \"type\": \"keyword\" },\n      \"transcription_accuracy\": { \"type\": \"float\" },\n      \"silence_ratio\":          { \"type\": \"float\" },\n      \"interruption_count\":     { \"type\": \"integer\" },\n      \"agent_talk_ratio\":       { \"type\": \"float\" },\n      \"pipeline\":               { \"type\": \"keyword\" },\n      \"language\":               { \"type\": \"keyword\" },\n      \"region\":                 { \"type\": \"keyword\" },\n      \"resolved\":               { \"type\": \"boolean\" }\n    }\n  }\n}\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"acknowledged\": true,\n  \"shards_acknowledged\": true,\n  \"index\": \"novapulse-calls\"\n}\n</code></pre>"},{"location":"kibana-lab/kibana-index-data/#step-3-index-documents","title":"Step 3: Index documents","text":"<p>The Elasticsearch bulk API indexes multiple documents in one request. Each document requires two lines: an action line and a document line.</p> <p>Bulk API formatting</p> <p>The bulk API requires each action and each document to be on a single unbroken line. Kibana Dev Tools auto-formats JSON across multiple lines when you paste it, which causes a parse error. The workaround is to index documents one at a time using <code>POST novapulse-calls/_doc</code> with standard multi-line JSON formatting, which Dev Tools handles correctly.</p> <p>Index each record individually:</p> <pre><code>POST novapulse-calls/_doc\n{\n  \"@timestamp\": \"2025-04-01T08:12:00Z\",\n  \"job_id\": \"job_001\",\n  \"agent_id\": \"AGT-12\",\n  \"team\": \"billing\",\n  \"call_type\": \"inbound\",\n  \"queue\": \"billing\",\n  \"duration_seconds\": 312,\n  \"overall_sentiment\": \"negative\",\n  \"sentiment_score\": -0.72,\n  \"sentiment_trend\": \"declining\",\n  \"transcription_accuracy\": 0.96,\n  \"silence_ratio\": 0.09,\n  \"interruption_count\": 4,\n  \"agent_talk_ratio\": 0.58,\n  \"pipeline\": \"full-analysis\",\n  \"language\": \"en-US\",\n  \"region\": \"emea\",\n  \"resolved\": false\n}\n</code></pre> <p>Repeat for all 12 records. The full dataset is available in the repository.</p>"},{"location":"kibana-lab/kibana-index-data/#step-4-verify-the-data","title":"Step 4: Verify the data","text":"<pre><code>GET novapulse-calls/_count\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"count\": 12\n}\n</code></pre>"},{"location":"kibana-lab/kibana-index-data/#step-5-create-a-data-view","title":"Step 5: Create a data view","text":"<p>Kibana requires a data view to connect to an Elasticsearch index.</p> <ol> <li>Go to Data Management (gear icon, bottom of left sidebar)</li> <li>Click Data Views, then Create data view</li> <li>Set Name to <code>NovaPulse Calls</code></li> <li>Set Index pattern to <code>novapulse-calls</code></li> <li>Set Timestamp field to <code>@timestamp</code></li> <li>Click Save data view to Kibana</li> </ol>"},{"location":"kibana-lab/kibana-index-data/#related","title":"Related","text":"<ul> <li>Explore with Discover</li> <li>Build a Lens dashboard</li> </ul>"},{"location":"kibana-lab/kibana-lens/","title":"Build a Lens dashboard","text":"<p>Kibana Lens is the default drag-and-drop visualization editor. It lets you build charts by dragging fields from a data view onto a workspace, and suggests the most appropriate visualization type based on the data. This page documents building a 3-panel NovaPulse call analytics dashboard.</p> <p>Official reference: Lens</p>"},{"location":"kibana-lab/kibana-lens/#create-the-dashboard","title":"Create the dashboard","text":"<ol> <li>Click Dashboards in the left sidebar</li> <li>Click Create dashboard</li> <li>Set the time range to April 1 to April 5, 2025</li> <li>Click Save and name it: <code>NovaPulse Call Analytics</code></li> </ol>"},{"location":"kibana-lab/kibana-lens/#visualization-1-sentiment-distribution-pie-chart","title":"Visualization 1: Sentiment Distribution (Pie chart)","text":"<p>This chart shows the proportion of calls by sentiment classification, giving a quick overview of overall call quality for the period.</p>"},{"location":"kibana-lab/kibana-lens/#build-it","title":"Build it","text":"<ol> <li>Click Add panel &gt; Create visualization</li> <li>In the chart type selector, choose Pie</li> <li>Drag overall_sentiment from the field list to the Slice by dimension</li> <li>Drag Records to the Metric dimension</li> <li>Click Save and return</li> <li>In the panel header, click the three-dot menu and select Edit panel title</li> <li>Title it: <code>Sentiment Distribution</code></li> </ol>"},{"location":"kibana-lab/kibana-lens/#what-you-see","title":"What you see","text":"<p>A pie chart with three slices: negative, neutral, and positive. With 12 sample records the split is 5 negative, 4 positive, 3 neutral.</p>"},{"location":"kibana-lab/kibana-lens/#why-this-chart-type","title":"Why this chart type","text":"<p>Pie charts work well for part-to-whole comparisons with 3 to 5 discrete categories. Sentiment classification has exactly three values, making this an appropriate choice. The proportions communicate call quality at a glance without requiring the viewer to read axis values.</p>"},{"location":"kibana-lab/kibana-lens/#visualization-2-daily-call-volume-by-team-stacked-bar-chart","title":"Visualization 2: Daily Call Volume by Team (Stacked bar chart)","text":"<p>This chart shows how many calls each team handled per day, stacked to show both individual team volume and daily totals.</p>"},{"location":"kibana-lab/kibana-lens/#build-it_1","title":"Build it","text":"<ol> <li>Click Add panel &gt; Create visualization</li> <li>Leave the chart type as Bar Stacked</li> <li>Drag @timestamp to the Horizontal axis</li> <li>Drag team to the Breakdown dimension</li> <li>Drag Records to the Vertical axis</li> <li>Click on @timestamp in the Horizontal axis and set the interval to 1 day</li> <li>Click Save and return</li> <li>Title it: <code>Daily Call Volume by Team</code></li> </ol>"},{"location":"kibana-lab/kibana-lens/#what-you-see_1","title":"What you see","text":"<p>A stacked bar chart with one bar per day, each divided by team color: billing, support, and sales. The height shows total daily call volume.</p>"},{"location":"kibana-lab/kibana-lens/#visualization-3-average-sentiment-score-by-agent-bar-chart","title":"Visualization 3: Average Sentiment Score by Agent (Bar chart)","text":"<p>This chart shows each agent's average sentiment score across all their calls, making it easy to identify which agents handle the most difficult calls or perform best.</p>"},{"location":"kibana-lab/kibana-lens/#build-it_2","title":"Build it","text":"<ol> <li>Click Add panel &gt; Create visualization</li> <li>Change the chart type to Bar vertical (or Bar)</li> <li>Drag agent_id to the Horizontal axis</li> <li>Click the agent_id dimension and set the function to Top values, number of values to 5</li> <li>Drag sentiment_score to the Vertical axis</li> <li>Click the sentiment_score dimension, set function to Average, and set the label to <code>Avg Sentiment Score</code></li> <li>Click Save and return</li> <li>Title it: <code>Average Sentiment Score by Agent</code></li> </ol>"},{"location":"kibana-lab/kibana-lens/#what-you-see_2","title":"What you see","text":"<p>A bar chart with one bar per agent. Bars above zero indicate a net-positive average sentiment across that agent's calls. Bars below zero indicate net-negative, those agents are handling the most difficult interactions or may need coaching support.</p> <p>In the sample data: AGT-42 performs best with an average around +0.43, while AGT-19 scores lowest at around -0.37.</p> <p></p> <p>Lens suggestions</p> <p>As you configure dimensions, Lens displays alternative visualization suggestions at the bottom of the editor. For the agent sentiment chart, Lens also suggested a data table showing exact scores per agent alongside the bar chart. Both are useful: the chart for quick visual comparison, the table for precise values. Consider adding both to a production dashboard.</p>"},{"location":"kibana-lab/kibana-lens/#inspect-the-underlying-elasticsearch-query","title":"Inspect the underlying Elasticsearch query","text":"<p>Every Lens visualization sends an Elasticsearch aggregation query behind the scenes. To see it:</p> <ol> <li>Hover over any panel on the dashboard</li> <li>Click the three-dot menu, then select Inspect</li> <li>Click Request</li> </ol> <p>For the sentiment pie chart, you will see a <code>terms</code> aggregation on the <code>overall_sentiment</code> field. For the agent sentiment chart, you will see an <code>avg</code> aggregation on <code>sentiment_score</code> grouped by <code>terms</code> on <code>agent_id</code>. This is useful for understanding how Lens translates visual configuration into Elasticsearch query DSL, and for replicating the same query via the API.</p>"},{"location":"kibana-lab/kibana-lens/#related","title":"Related","text":"<ul> <li>Explore with Discover</li> <li>Index NovaPulse data</li> <li>Kibana Lens documentation</li> <li>Supported chart types</li> </ul>"},{"location":"reference/error-codes/","title":"Error Codes","text":"<p>All NovaPulse API errors return a consistent JSON structure with an HTTP status code, a machine-readable error code, and a human-readable message.</p>"},{"location":"reference/error-codes/#error-response-format","title":"Error response format","text":"<pre><code>{\n  \"error\": {\n    \"code\": \"invalid_api_key\",\n    \"message\": \"The API key provided is invalid or has been revoked.\",\n    \"status\": 401\n  }\n}\n</code></pre> Field Type Description <code>code</code> string Machine-readable error identifier. Use this in your error handling logic. <code>message</code> string Human-readable description of the error. <code>status</code> integer The HTTP status code returned with this error."},{"location":"reference/error-codes/#authentication-errors","title":"Authentication errors","text":"HTTP status Code Cause Resolution <code>401</code> <code>missing_api_key</code> No <code>Authorization</code> header was sent. Include <code>Authorization: Bearer YOUR_KEY</code> in every request. <code>401</code> <code>invalid_api_key</code> The key is malformed or does not exist. Verify the key in your dashboard under Settings &gt; API Keys. <code>401</code> <code>expired_api_key</code> The key has been revoked or expired. Generate a new key in the dashboard. <code>403</code> <code>insufficient_permissions</code> The key does not have the required scope for this endpoint. Check the key's permission scope or generate a key with broader permissions."},{"location":"reference/error-codes/#request-errors","title":"Request errors","text":"HTTP status Code Cause Resolution <code>400</code> <code>missing_required_field</code> A required field was not included in the request. Check the API reference for required fields and include them. <code>400</code> <code>invalid_parameter</code> A parameter value is not valid (wrong type, out of range, or unsupported value). Review accepted values in the endpoint's parameter table. <code>400</code> <code>invalid_language_code</code> The <code>language</code> value is not a valid BCP-47 tag. Use a supported language code such as <code>en-US</code>. <code>413</code> <code>file_too_large</code> The uploaded file exceeds the 500 MB limit. Split the audio file and upload in parts. <code>415</code> <code>unsupported_media_type</code> The audio format is not supported. Convert the file to MP3, WAV, M4A, FLAC, or OGG before uploading."},{"location":"reference/error-codes/#job-and-resource-errors","title":"Job and resource errors","text":"HTTP status Code Cause Resolution <code>404</code> <code>job_not_found</code> The job ID does not exist or belongs to a different account. Verify the job ID returned at upload time. <code>404</code> <code>job_not_ready</code> The job is still processing. Results are not yet available. Poll job status until <code>\"status\": \"completed\"</code> before retrieving results. <code>403</code> <code>pipeline_mismatch</code> The requested insight type requires <code>full-analysis</code> but the job was processed with <code>transcription</code> only. Re-upload the file with <code>pipeline=full-analysis</code>. <code>409</code> <code>job_already_exists</code> A job with identical file content and metadata was submitted within the deduplication window (60 seconds). Wait 60 seconds before resubmitting, or modify the metadata to differentiate the job."},{"location":"reference/error-codes/#rate-limit-errors","title":"Rate limit errors","text":"HTTP status Code Cause Resolution <code>429</code> <code>rate_limit_exceeded</code> The request rate for your API key exceeded the allowed limit. Wait for the duration specified in the <code>Retry-After</code> header before retrying. See Rate Limits. <code>429</code> <code>concurrent_jobs_exceeded</code> Too many jobs are processing simultaneously for your account tier. Wait for at least one job to complete before submitting new uploads."},{"location":"reference/error-codes/#server-errors","title":"Server errors","text":"HTTP status Code Cause Resolution <code>500</code> <code>internal_error</code> An unexpected error occurred on the NovaPulse server. Retry the request with exponential backoff. If the issue persists, contact support with your job ID. <code>503</code> <code>service_unavailable</code> The service is temporarily unavailable, usually due to maintenance. Check the NovaPulse status page and retry after the maintenance window."},{"location":"reference/error-codes/#retry-guidance","title":"Retry guidance","text":"<p>For transient errors (429, 500, 503), use exponential backoff:</p> <pre><code>import time\nimport requests\n\ndef request_with_retry(url, headers, max_retries=4):\n    delay = 1\n    for attempt in range(max_retries):\n        response = requests.get(url, headers=headers)\n        if response.status_code == 429:\n            retry_after = int(response.headers.get(\"Retry-After\", delay))\n            time.sleep(retry_after)\n            delay *= 2\n        elif response.status_code &gt;= 500:\n            time.sleep(delay)\n            delay *= 2\n        else:\n            return response\n    raise Exception(\"Max retries exceeded\")\n</code></pre>"},{"location":"reference/error-codes/#related","title":"Related","text":"<ul> <li>Rate Limits</li> <li>Authentication</li> <li>Audio Ingestion API</li> </ul>"},{"location":"reference/glossary/","title":"Glossary","text":"<p>Terms specific to NovaPulse and the AI/search concepts it uses.</p>"},{"location":"reference/glossary/#dense-vector","title":"Dense vector","text":"<p>A fixed-length array of floating-point numbers that encodes the semantic meaning of a piece of text. In NovaPulse, every transcript segment is stored as a dense vector to enable semantic search. Contrast with a sparse vector, which stores only non-zero values.</p> <p>See also: Vector Search</p>"},{"location":"reference/glossary/#embedding-model","title":"Embedding model","text":"<p>A machine learning model that converts text into a dense vector. NovaPulse uses an embedding model to represent transcript segments as vectors so that semantically similar segments produce numerically similar vectors.</p>"},{"location":"reference/glossary/#knn-search","title":"kNN search","text":"<p>k-nearest neighbor search. A method for finding the k items in a dataset whose vectors are most similar to a query vector. NovaPulse uses kNN search to retrieve transcript segments that are semantically closest to a natural language query.</p> <p>See also: Vector Search</p>"},{"location":"reference/glossary/#cosine-similarity","title":"Cosine similarity","text":"<p>A measure of similarity between two vectors calculated as the cosine of the angle between them. A cosine similarity of 1 means identical direction (same meaning). A value of 0 means orthogonal (unrelated). Used by NovaPulse's vector search to rank results by semantic relevance.</p>"},{"location":"reference/glossary/#bm25","title":"BM25","text":"<p>Best Match 25. The classic probabilistic keyword ranking algorithm used by most search engines. BM25 scores documents based on term frequency and inverse document frequency. Strong for exact term matches; weak for conceptual or conversational queries.</p>"},{"location":"reference/glossary/#hybrid-search","title":"Hybrid search","text":"<p>A search strategy that combines BM25 keyword ranking with kNN semantic search, merging results using Reciprocal Rank Fusion (RRF). NovaPulse uses hybrid search to handle both precise term queries and conversational natural language queries.</p> <p>See also: Vector Search</p>"},{"location":"reference/glossary/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<p>An AI architecture that enhances LLM responses by first retrieving relevant documents from a knowledge base and including them as context in the prompt. RAG prevents hallucinations and enables LLMs to answer questions about private or current data they were not trained on.</p> <p>See also: RAG Pipeline</p>"},{"location":"reference/glossary/#speaker-diarization","title":"Speaker diarization","text":"<p>The process of automatically identifying and labeling different speakers in an audio recording. NovaPulse's diarization engine assigns each speech segment to a speaker label (Agent, Customer, or Speaker N) without requiring prior voice enrollment.</p>"},{"location":"reference/glossary/#pii-personally-identifiable-information","title":"PII (Personally Identifiable Information)","text":"<p>Any data that can identify a specific individual: names, phone numbers, email addresses, account numbers, and similar. NovaPulse's full-analysis pipeline includes an automated PII detection and redaction module that masks PII in transcripts before storage.</p>"},{"location":"reference/glossary/#sentiment-analysis","title":"Sentiment analysis","text":"<p>The automated classification of text by emotional tone. NovaPulse produces per-segment sentiment scores (ranging from -1.0 negative to +1.0 positive) and an overall call sentiment classification.</p>"},{"location":"reference/glossary/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<p>An NLP technique for identifying and classifying named entities in text,  people, organizations, products, locations, and dates. NovaPulse uses NER as part of its PII detection pipeline and for extracting structured information from unstructured conversation data.</p>"},{"location":"reference/glossary/#elser","title":"ELSER","text":"<p>Elastic Learned Sparse Encoder. Elastic's proprietary semantic search model that produces sparse vector representations optimized for English-language retrieval. Unlike dense embedding models, ELSER does not require a separate embedding service.</p>"},{"location":"reference/glossary/#inference-api","title":"Inference API","text":"<p>An API that connects a search or data platform to external machine learning models (such as OpenAI, Cohere, or Hugging Face models) to generate embeddings, run NLP tasks, or call LLMs as part of a search or data pipeline.</p>"},{"location":"reference/rate-limits/","title":"Rate Limits","text":"<p>NovaPulse enforces rate limits per API key to ensure fair usage and platform stability. All limits are measured on a rolling 60-second window.</p>"},{"location":"reference/rate-limits/#limits-by-endpoint","title":"Limits by endpoint","text":"Endpoint Method Limit <code>/v1/audio/ingest</code> POST 60 requests per minute <code>/v1/jobs/{job_id}</code> GET 120 requests per minute <code>/v1/transcriptions/{job_id}</code> GET 120 requests per minute <code>/v1/insights/{job_id}</code> GET 120 requests per minute"},{"location":"reference/rate-limits/#concurrent-job-limits","title":"Concurrent job limits","text":"<p>In addition to request rate limits, NovaPulse limits the number of jobs that can be actively processing at the same time per account:</p> Account tier Concurrent jobs Free trial 3 Starter 10 Professional 50 Enterprise Custom"},{"location":"reference/rate-limits/#rate-limit-response-headers","title":"Rate limit response headers","text":"<p>Every API response includes headers showing your current rate limit status:</p> Header Description <code>X-RateLimit-Limit</code> The maximum number of requests allowed per minute for this endpoint. <code>X-RateLimit-Remaining</code> The number of requests remaining in the current window. <code>X-RateLimit-Reset</code> Unix timestamp when the current window resets. <code>Retry-After</code> Seconds to wait before retrying. Only present on <code>429</code> responses."},{"location":"reference/rate-limits/#example-headers-on-a-normal-response","title":"Example headers on a normal response","text":"<pre><code>X-RateLimit-Limit: 60\nX-RateLimit-Remaining: 43\nX-RateLimit-Reset: 1744714380\n</code></pre>"},{"location":"reference/rate-limits/#example-headers-on-a-rate-limited-response","title":"Example headers on a rate-limited response","text":"<pre><code>HTTP/1.1 429 Too Many Requests\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1744714380\nRetry-After: 12\n</code></pre>"},{"location":"reference/rate-limits/#handling-rate-limit-errors","title":"Handling rate limit errors","text":"<p>When you receive a <code>429</code> response, read the <code>Retry-After</code> header and wait that many seconds before retrying. Do not retry immediately.</p> <pre><code>import time\nimport requests\n\ndef upload_with_rate_limit_handling(file_path, headers):\n    with open(file_path, 'rb') as f:\n        files = {'file': f}\n        response = requests.post(\n            'https://api.novapulse.io/v1/audio/ingest',\n            headers=headers,\n            files=files,\n            data={'pipeline': 'full-analysis'}\n        )\n\n    if response.status_code == 429:\n        retry_after = int(response.headers.get('Retry-After', 10))\n        print(f\"Rate limited. Retrying in {retry_after} seconds.\")\n        time.sleep(retry_after)\n        return upload_with_rate_limit_handling(file_path, headers)\n\n    return response.json()\n</code></pre>"},{"location":"reference/rate-limits/#strategies-for-high-volume-integrations","title":"Strategies for high-volume integrations","text":"<p>If you are processing large batches of audio files, use these strategies to stay within rate limits:</p> <p>Use webhooks instead of polling. Polling job status repeatedly consumes your request quota. Pass <code>webhook_url</code> at upload time and let NovaPulse notify you when each job completes.</p> <p>Implement a queue. Rather than uploading all files simultaneously, queue uploads and process them at a controlled rate. A rate of 50 uploads per minute gives you headroom below the 60/minute limit.</p> <p>Monitor the remaining header. Read <code>X-RateLimit-Remaining</code> on every response and slow down proactively before hitting zero, rather than reacting to <code>429</code> errors.</p> <p>Batch by priority. For large backlog processing, separate urgent calls (escalations, complaints) from routine processing and upload high-priority files first.</p>"},{"location":"reference/rate-limits/#requesting-a-limit-increase","title":"Requesting a limit increase","text":"<p>If your integration consistently requires higher limits, contact NovaPulse support to discuss an Enterprise plan with custom rate limits and concurrent job allocations tailored to your volume.</p>"},{"location":"reference/rate-limits/#related","title":"Related","text":"<ul> <li>Error Codes \u2014 Full error reference including <code>429</code> handling</li> <li>Audio Ingestion API</li> <li>Upload and Analyze tutorial</li> </ul>"},{"location":"release-notes/changelog/","title":"Changelog","text":"<p>All notable changes to the NovaPulse API and platform are documented here. Dates are in ISO 8601 format. Changes are grouped by release and ordered from most recent to oldest.</p> <p>For full release notes on each version, follow the links below.</p>"},{"location":"release-notes/changelog/#versioning-policy","title":"Versioning policy","text":"<p>NovaPulse follows semantic versioning:</p> Change type Example What it means Major v1 to v2 Breaking changes. A migration guide is published. Minimum 90-day deprecation notice. Minor v1.0 to v1.1 New features added in a backwards-compatible way. No action required. Patch v1.0.0 to v1.0.1 Backwards-compatible bug fixes and performance improvements."},{"location":"release-notes/changelog/#unreleased","title":"Unreleased","text":"<p>Changes that are merged and staged but not yet in a published release.</p> <ul> <li>Real-time audio stream endpoint (Phase 2 beta)</li> <li>Support for additional languages: Spanish (<code>es-ES</code>), French (<code>fr-FR</code>)</li> <li>Speaker enrollment API for improved diarization accuracy</li> <li>Per-minute sentiment granularity via <code>granularity=minute</code> query parameter</li> </ul>"},{"location":"release-notes/changelog/#v100-2025-05-01","title":"v1.0.0 \u2014 2025-05-01","text":"<p>Initial General Availability release of the NovaPulse AI Voice Analytics Platform.</p> <p>New features:</p> <ul> <li><code>POST /v1/audio/ingest</code>: Upload audio files for processing</li> <li><code>GET /v1/jobs/{job_id}</code>: Check processing job status</li> <li><code>GET /v1/transcriptions/{job_id}</code>: Retrieve speaker-attributed transcripts</li> <li><code>GET /v1/insights/{job_id}</code>: Retrieve sentiment analysis and behavioral metrics</li> <li>Webhook notifications on job completion via <code>webhook_url</code> parameter</li> <li>Automated PII detection and redaction on all transcripts</li> <li>AI-generated conversation summaries in <code>full-analysis</code> pipeline</li> <li>Export formats: JSON, SRT, VTT, TXT</li> </ul> <p>Supported audio formats: MP3, WAV, M4A, FLAC, OGG</p> <p>Known limitations: - English only (<code>en-US</code>) - Maximum file size: 500 MB - No real-time streaming</p> <p>Full release notes</p>"},{"location":"release-notes/changelog/#how-to-stay-informed","title":"How to stay informed","text":"<p>Watch the novapulse-docs repository on GitHub to receive notifications when the changelog is updated.</p>"},{"location":"release-notes/v1.0.0/","title":"NovaPulse v1.0.0","text":"<p>Release date: May 1, 2025 Type: General Availability API version: v1</p> <p>This is the first general availability release of the NovaPulse AI Voice Analytics Platform. This release delivers the Phase 1 capabilities: structured intelligence from recorded audio.</p>"},{"location":"release-notes/v1.0.0/#new-features","title":"New features","text":""},{"location":"release-notes/v1.0.0/#audio-ingestion","title":"Audio ingestion","text":"<p>Upload audio files in MP3, WAV, M4A, FLAC, or OGG format via <code>POST /v1/audio/ingest</code>. Files up to 500 MB are supported. Documentation</p>"},{"location":"release-notes/v1.0.0/#transcription-with-speaker-diarization","title":"Transcription with speaker diarization","text":"<p>Automated, speaker-attributed transcription with \u226590% accuracy. Transcripts include per-segment timestamps, speaker labels, and confidence scores. Documentation</p>"},{"location":"release-notes/v1.0.0/#sentiment-analysis","title":"Sentiment analysis","text":"<p>Per-segment and per-call sentiment scoring with \u226585% reliability. Scores range from -1.0 (strongly negative) to +1.0 (strongly positive). Documentation</p>"},{"location":"release-notes/v1.0.0/#pii-detection-and-redaction","title":"PII detection and redaction","text":"<p>Automated detection of names, phone numbers, email addresses, and account numbers. PII is redacted in stored transcripts by default. Configurable per API request via the <code>redaction</code> parameter.</p>"},{"location":"release-notes/v1.0.0/#webhook-notifications","title":"Webhook notifications","text":"<p>Pass <code>webhook_url</code> in any ingest request to receive a POST callback when processing completes. Eliminates the need for polling.</p>"},{"location":"release-notes/v1.0.0/#ai-summarization","title":"AI summarization","text":"<p>Automatic conversation summaries included in full-analysis results, covering key topics, outcomes, and recommended follow-up actions.</p>"},{"location":"release-notes/v1.0.0/#known-limitations","title":"Known limitations","text":"<ul> <li> <p>English only. The v1.0 release supports English-language audio (<code>en-US</code>).   Additional language support is planned for v1.1.</p> </li> <li> <p>No on-premise deployment. NovaPulse is exclusively cloud-hosted   in this release. On-premise options will be evaluated based on   enterprise client requirements.</p> </li> <li> <p>Maximum file size: 500 MB. Files larger than 500 MB must be   split before upload.</p> </li> <li> <p>No real-time streaming. Real-time audio stream processing   (Phase 2) is not included in this release.</p> </li> </ul>"},{"location":"release-notes/v1.0.0/#api-changes","title":"API changes","text":"<p>This is the initial stable release. The <code>v1</code> API is considered stable. Breaking changes will not be introduced without a version increment and a minimum 90-day deprecation notice.</p>"},{"location":"release-notes/v1.0.0/#system-requirements","title":"System requirements","text":"Requirement Value Supported audio formats MP3, WAV, M4A, FLAC, OGG Maximum file size 500 MB API version v1 Authentication Bearer token (API key) Rate limit 100 requests/minute per key"},{"location":"release-notes/v1.0.0/#compliance-and-security","title":"Compliance and security","text":"<ul> <li>GDPR-ready: data processing agreements available on request</li> <li>Encryption in transit: TLS 1.3</li> <li>Encryption at rest: AES-256</li> <li>PII redaction: enabled by default in all transcripts</li> <li>Data retention: 90 days by default; configurable per account</li> <li>Multi-tenant isolation: row-level security across all data stores</li> </ul>"},{"location":"tutorials/getting-started/","title":"Getting started","text":"<p>This tutorial walks you through uploading an audio file to NovaPulse and retrieving a transcript with sentiment analysis. By the end, you will have made your first successful API request and received structured insights from a real audio file.</p> <p>Time to complete: approximately 10 minutes</p>"},{"location":"tutorials/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin:</p> <ul> <li> A NovaPulse account - sign up free</li> <li> An API key - see Authentication</li> <li> cURL installed (included on Windows 10+ and macOS)</li> <li> An audio file in MP3 or WAV format (minimum 10 seconds)</li> </ul>"},{"location":"tutorials/getting-started/#step-1-set-your-api-key-as-an-environment-variable","title":"Step 1: Set your API key as an environment variable","text":"<p>Store your key as an environment variable to avoid hardcoding it in commands. Open your terminal and run:</p> macOS / LinuxWindows (PowerShell)Windows (Command Prompt) <pre><code>export NOVAPULSE_API_KEY=nvp_live_YOUR_KEY_HERE\n</code></pre> <pre><code>$env:NOVAPULSE_API_KEY = 'nvp_live_YOUR_KEY_HERE'\n</code></pre> <pre><code>set NOVAPULSE_API_KEY=nvp_live_YOUR_KEY_HERE\n</code></pre>"},{"location":"tutorials/getting-started/#step-2-upload-an-audio-file","title":"Step 2: Upload an audio file","text":"<p>Upload your file to the ingestion endpoint. Replace <code>call.mp3</code> with the actual path to your audio file.</p> <pre><code>curl -X POST https://api.novapulse.io/v1/audio/ingest \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\" \\\n  -F 'file=@call.mp3' \\\n  -F 'pipeline=full-analysis'\n</code></pre> <p>You should see a response like this:</p> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"status\": \"queued\",\n  \"created_at\": \"2025-04-15T14:32:00Z\",\n  \"estimated_completion\": \"2025-04-15T14:34:00Z\"\n}\n</code></pre> <p>Save the <code>job_id</code> value, you will use it in the next steps.</p>"},{"location":"tutorials/getting-started/#step-3-check-processing-status","title":"Step 3: Check processing status","text":"<p>Audio processing is asynchronous. Poll the job status endpoint until <code>status</code> is <code>completed</code>:</p> <pre><code>curl https://api.novapulse.io/v1/jobs/job_9f3c2a1b4e7d \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"status\": \"completed\",\n  \"pipeline\": \"full-analysis\",\n  \"completed_at\": \"2025-04-15T14:33:47Z\"\n}\n</code></pre> <p>Tip</p> <p>For production integrations, use a webhook instead of polling. Pass <code>webhook_url</code> in your upload request and NovaPulse will POST a notification to your endpoint when processing completes.</p>"},{"location":"tutorials/getting-started/#step-4-retrieve-the-transcript","title":"Step 4: Retrieve the transcript","text":"<pre><code>curl https://api.novapulse.io/v1/transcriptions/job_9f3c2a1b4e7d \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <p>The response includes a speaker-attributed transcript:</p> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"language\": \"en-US\",\n  \"duration_seconds\": 312,\n  \"segments\": [\n    {\n      \"speaker\": \"Agent\",\n      \"start\": 0.0,\n      \"end\": 4.2,\n      \"text\": \"Thank you for calling support. How can I help you today?\"\n    },\n    {\n      \"speaker\": \"Customer\",\n      \"start\": 4.5,\n      \"end\": 11.8,\n      \"text\": \"I need to cancel my subscription. The pricing just doesn't work for us.\"\n    }\n  ]\n}\n</code></pre>"},{"location":"tutorials/getting-started/#step-5-retrieve-sentiment-analysis","title":"Step 5: Retrieve sentiment analysis","text":"<pre><code>curl https://api.novapulse.io/v1/insights/job_9f3c2a1b4e7d?type=sentiment \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <pre><code>{\n  \"job_id\": \"job_9f3c2a1b4e7d\",\n  \"overall_sentiment\": \"negative\",\n  \"sentiment_score\": -0.68,\n  \"segments\": [\n    { \"speaker\": \"Customer\", \"start\": 4.5, \"sentiment\": \"negative\", \"score\": -0.82 },\n    { \"speaker\": \"Agent\",    \"start\": 12.1, \"sentiment\": \"neutral\",  \"score\": 0.05 }\n  ]\n}\n</code></pre>"},{"location":"tutorials/getting-started/#what-you-built","title":"What you built","text":"<ul> <li> Uploaded an audio file using the Ingestion API</li> <li> Polled job status and confirmed completion</li> <li> Retrieved a speaker-attributed transcript</li> <li> Retrieved per-segment sentiment analysis</li> </ul>"},{"location":"tutorials/getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Upload &amp; Analyze Audio tutorial \u2014 batch processing   and metadata strategies</li> <li>Sentiment Analysis API reference</li> <li>Vector Search concepts \u2014 how to query by meaning</li> </ul>"},{"location":"tutorials/upload-and-analyze/","title":"Upload and analyze audio","text":"<p>This tutorial covers the full NovaPulse analysis workflow in depth. You will upload an audio file, configure the analysis pipeline, retrieve the complete results: transcript, sentiment, behavioral metrics, and AI summary, and learn how to use metadata to organize and filter your data at scale.</p> <p>Time to complete: approximately 20 minutes</p>"},{"location":"tutorials/upload-and-analyze/#prerequisites","title":"Prerequisites","text":"<p>Before you begin:</p> <ul> <li> Completed the Getting Started tutorial</li> <li> A NovaPulse API key set as an environment variable</li> <li> At least one audio file in MP3 or WAV format</li> <li> cURL or Python installed</li> </ul> <p>If you haven't set your API key as an environment variable yet:</p> macOS / LinuxWindows (PowerShell) <pre><code>export NOVAPULSE_API_KEY=nvp_live_YOUR_KEY_HERE\n</code></pre> <pre><code>$env:NOVAPULSE_API_KEY = 'nvp_live_YOUR_KEY_HERE'\n</code></pre>"},{"location":"tutorials/upload-and-analyze/#step-1-upload-with-full-metadata","title":"Step 1: Upload with full metadata","text":"<p>Metadata is key-value data you attach to a job at upload time. It has no effect on processing, but it makes your data filterable and traceable in the dashboard and via the API. It is essential for production use.</p> <p>Upload your audio file with a rich metadata payload:</p> <pre><code>curl -X POST https://api.novapulse.io/v1/audio/ingest \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\" \\\n  -F 'file=@support-call-april.mp3' \\\n  -F 'pipeline=full-analysis' \\\n  -F 'language=en-US' \\\n  -F 'metadata={\n    \"agent_id\": \"AGT-42\",\n    \"team\": \"customer-support\",\n    \"call_type\": \"inbound\",\n    \"queue\": \"billing\",\n    \"crm_ticket_id\": \"TKT-9821\",\n    \"region\": \"emea\"\n  }' \\\n  -F 'webhook_url=https://your-system.io/novapulse/webhook'\n</code></pre> <p>You should receive:</p> <pre><code>{\n  \"job_id\": \"job_7a2d9c4f1e8b\",\n  \"status\": \"queued\",\n  \"pipeline\": \"full-analysis\",\n  \"created_at\": \"2025-04-15T09:14:22Z\",\n  \"estimated_completion\": \"2025-04-15T09:16:30Z\",\n  \"file\": {\n    \"name\": \"support-call-april.mp3\",\n    \"size_bytes\": 6241088,\n    \"duration_seconds\": 389\n  }\n}\n</code></pre> <p>Save the <code>job_id</code>. You will use it throughout this tutorial.</p> <p>Metadata best practices</p> <p>Use consistent key names across all uploads, this makes filtering reliable. Treat metadata like database columns: define a schema and stick to it. Common fields: <code>agent_id</code>, <code>team</code>, <code>call_type</code>, <code>queue</code>, <code>crm_ticket_id</code>, <code>region</code>.</p>"},{"location":"tutorials/upload-and-analyze/#step-2-use-a-webhook-instead-of-polling-recommended","title":"Step 2: Use a webhook instead of polling (recommended)","text":"<p>In the Getting Started tutorial you polled the job status endpoint. For production integrations, webhooks are better: your system receives a notification the moment processing completes, with no need for repeated requests.</p> <p>If you passed <code>webhook_url</code> in Step 1, NovaPulse will POST this payload to your endpoint when the job finishes:</p> <pre><code>{\n  \"event\": \"job.completed\",\n  \"job_id\": \"job_7a2d9c4f1e8b\",\n  \"status\": \"completed\",\n  \"pipeline\": \"full-analysis\",\n  \"completed_at\": \"2025-04-15T09:16:18Z\"\n}\n</code></pre> <p>Your webhook handler should respond with <code>200 OK</code> within 5 seconds. NovaPulse retries failed webhook deliveries up to 3 times with exponential backoff.</p> <p>If you don't have a webhook endpoint available during development, continue to use polling:</p> <pre><code>curl https://api.novapulse.io/v1/jobs/job_7a2d9c4f1e8b \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <p>Wait until <code>\"status\": \"completed\"</code> before proceeding.</p>"},{"location":"tutorials/upload-and-analyze/#step-3-retrieve-the-full-transcript","title":"Step 3: Retrieve the full transcript","text":"<pre><code>curl https://api.novapulse.io/v1/transcriptions/job_7a2d9c4f1e8b \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <p>The response includes speaker-attributed segments with confidence scores:</p> <pre><code>{\n  \"job_id\": \"job_7a2d9c4f1e8b\",\n  \"language\": \"en-US\",\n  \"duration_seconds\": 389,\n  \"speakers_detected\": 2,\n  \"transcription_accuracy\": 0.95,\n  \"segments\": [\n    {\n      \"id\": 1,\n      \"speaker\": \"Agent\",\n      \"start\": 0.0,\n      \"end\": 5.1,\n      \"text\": \"Thank you for calling billing support. My name is Sarah, how can I help?\",\n      \"confidence\": 0.99\n    },\n    {\n      \"id\": 2,\n      \"speaker\": \"Customer\",\n      \"start\": 5.4,\n      \"end\": 14.7,\n      \"text\": \"Hi Sarah, I got charged twice this month and nobody seems to know why.\",\n      \"confidence\": 0.97\n    }\n  ],\n  \"summary\": \"Customer reported a duplicate billing charge. Agent investigated the account, confirmed the error, and initiated a refund. Resolved \u2014 refund processing within 3-5 business days.\",\n  \"pii_redacted\": true\n}\n</code></pre>"},{"location":"tutorials/upload-and-analyze/#export-as-srt-subtitle-format","title":"Export as SRT (subtitle format)","text":"<p>For integrations with video players, training platforms, or compliance archives, export the transcript as SRT:</p> <pre><code>curl \"https://api.novapulse.io/v1/transcriptions/job_7a2d9c4f1e8b?format=srt\" \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\" \\\n  -o transcript.srt\n</code></pre>"},{"location":"tutorials/upload-and-analyze/#step-4-retrieve-sentiment-analysis","title":"Step 4: Retrieve sentiment analysis","text":"<pre><code>curl \"https://api.novapulse.io/v1/insights/job_7a2d9c4f1e8b?type=sentiment\" \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <pre><code>{\n  \"job_id\": \"job_7a2d9c4f1e8b\",\n  \"sentiment\": {\n    \"overall_label\": \"positive\",\n    \"overall_score\": 0.42,\n    \"trend\": \"improving\",\n    \"segments\": [\n      {\n        \"segment_id\": 2,\n        \"speaker\": \"Customer\",\n        \"start\": 5.4,\n        \"end\": 14.7,\n        \"label\": \"negative\",\n        \"score\": -0.71\n      },\n      {\n        \"segment_id\": 4,\n        \"speaker\": \"Customer\",\n        \"start\": 47.2,\n        \"end\": 58.9,\n        \"label\": \"positive\",\n        \"score\": 0.84\n      }\n    ]\n  }\n}\n</code></pre> <p>Notice the <code>trend</code> field is <code>improving</code>. The customer started frustrated but ended satisfied. This is a high-value signal for agent coaching: the agent successfully de-escalated the situation.</p>"},{"location":"tutorials/upload-and-analyze/#step-5-retrieve-behavioral-metrics","title":"Step 5: Retrieve behavioral metrics","text":"<pre><code>curl \"https://api.novapulse.io/v1/insights/job_7a2d9c4f1e8b?type=behavioral\" \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <pre><code>{\n  \"job_id\": \"job_7a2d9c4f1e8b\",\n  \"behavioral\": {\n    \"silence_ratio\": 0.06,\n    \"interruption_count\": 1,\n    \"agent_talk_ratio\": 0.55,\n    \"customer_talk_ratio\": 0.39,\n    \"average_response_time_seconds\": 1.1\n  },\n  \"recommendations\": [\n    \"Sentiment trend improved significantly \u2014 flag for positive agent coaching example.\",\n    \"Low interruption count indicates calm, professional interaction.\",\n    \"Agent talk ratio slightly high \u2014 monitor for over-explanation patterns.\"\n  ]\n}\n</code></pre>"},{"location":"tutorials/upload-and-analyze/#step-6-retrieve-everything-in-one-request","title":"Step 6: Retrieve everything in one request","text":"<p>Instead of calling each endpoint separately, use <code>type=all</code> to retrieve sentiment, behavioral metrics, summary, and recommendations in a single response:</p> <pre><code>curl \"https://api.novapulse.io/v1/insights/job_7a2d9c4f1e8b?type=all\" \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <p>This reduces latency for dashboards and reporting pipelines that need the full picture at once.</p>"},{"location":"tutorials/upload-and-analyze/#step-7-filter-by-customer-sentiment-only","title":"Step 7: Filter by customer sentiment only","text":"<p>When building agent performance dashboards, you often only care about the customer's sentiment, not the agent's. Use the <code>speaker</code> filter:</p> <pre><code>curl \"https://api.novapulse.io/v1/insights/job_7a2d9c4f1e8b?type=sentiment&amp;speaker=Customer\" \\\n  -H \"Authorization: Bearer $NOVAPULSE_API_KEY\"\n</code></pre> <p>This returns only the customer's sentiment segments, making it straightforward to calculate a customer satisfaction score (CSAT) from actual conversation data rather than post-call surveys.</p>"},{"location":"tutorials/upload-and-analyze/#what-you-built","title":"What you built","text":"<ul> <li> Uploaded audio with structured metadata for production-grade traceability</li> <li> Configured webhook notification instead of polling</li> <li> Retrieved a speaker-attributed transcript with confidence scores</li> <li> Exported the transcript in SRT format</li> <li> Retrieved per-segment sentiment with trend analysis</li> <li> Retrieved behavioral metrics and AI-generated recommendations</li> <li> Retrieved all insights in a single request</li> <li> Filtered sentiment by speaker for CSAT analysis</li> </ul>"},{"location":"tutorials/upload-and-analyze/#handling-errors-in-production","title":"Handling errors in production","text":"<p>Two errors you will commonly encounter:</p> <p>Job still processing - 404:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"job_not_ready\",\n    \"message\": \"The job is still processing. Retry after the estimated completion time.\",\n    \"status\": 404\n  }\n}\n</code></pre> <p>Always check job status before retrieving results. Do not assume a job is complete based on elapsed time alone.</p> <p>Pipeline mismatch - 403:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"pipeline_mismatch\",\n    \"message\": \"Sentiment analysis requires pipeline=full-analysis. This job was processed with pipeline=transcription.\",\n    \"status\": 403\n  }\n}\n</code></pre> <p>Sentiment and behavioral data are only available for jobs processed with <code>pipeline=full-analysis</code>. If you need to re-process, upload the file again with the correct pipeline.</p> <p>Re-processing audio</p> <p>NovaPulse does not currently support re-processing an existing job with a different pipeline. Upload the file again with <code>pipeline=full-analysis</code> to get sentiment and behavioral data for a file previously processed with <code>pipeline=transcription</code>.</p>"},{"location":"tutorials/upload-and-analyze/#next-steps","title":"Next steps","text":"<ul> <li>Sentiment Analysis API reference \u2014   full response schema and parameter reference</li> <li>Transcription API reference \u2014   export formats and word-level timestamps</li> <li>Vector Search concepts \u2014   how to search across transcripts by meaning</li> <li>RAG Pipeline \u2014   how NovaPulse answers natural language questions about your data</li> </ul>"}]}